version: 2

models:
  - name: int_gtfs_schedule__joined_feed_outcomes
    description: |
      Each row is an individual download attempt combined with the associated unzip and parse attempts for
      a given feed on a given date.
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - base64_url
            - ts
    columns:
      - name: ts
      - name: base64_url
      - name: _config_extract_ts
      - name: download_success
      - name: download_exception
      - name: unzip_success
      - name: unzip_exception
      - name: zipfile_extract_md5hash
      - name: zipfile_files
      - name: zipfile_dirs
      - name: pct_files_successfully_parsed
  - name: int_gtfs_schedule__grouped_feed_file_parse_outcomes
    description: |
      Each row is a feed (URL + timestamp), with a summary of whether parsing was successful on the constituent files
      within that feed.
      "Success" means that the raw input file (usually a .txt file) was converted to JSONL format without an error.
      This does not guarantee validity according to the GTFS specification, just that the original input file was
      parseable on a pure file-format level (i.e., the file was not corrupt.)
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - base64_url
            - ts
    columns:
      - name: base64_url
      - name: ts
      - name: count_successes
        description: |
          Number of successful file conversions within this feed.
      - name: count_files
        description: |
          Number of total file conversions attempted for this feed.
      - name: pct_success
        description: |
          Successes as a percent of total files (`count_successes` / `count_files` * 100).
          If this is a value less than 100, indicates that file conversion failed for some files in this feed.

  - name: int_gtfs_schedule__long_calendar
    description: |
      This table transforms the raw GTFS calendar.txt format (where each row corresponds to a `service_id` and
      each day of the week is a column and service indicators are entered in a "wide" fashion)
      into a long format, where a row is identified by `feed_key`, `service_id`, and `date`.
    columns:
      - &schedule_key
        name: key
        description:
          Synthetic key from `service_id`, `feed_key`, and `service_date`.
        tests:
          - unique
          - not_null
      - name: feed_key
        description: Foreign key for `dim_schedule_feeds`.
      - name: service_id
        description: '{{ doc("gtfs_calendar__service_id") }}'
      - name: service_date
        description: |
          Date on which this service was active (i.e., this date is betweem the
          `start_date` and `end_date` for this service).
      - name: day_num
        description: |
          Day of week as number (Sunday = 1, Saturday = 7).
      - name: service_bool
        description: |
          Boolean indicating whether there is service for this `service_id` / `date` pair.
        tests:
          - not_null
      - name: calendar_key
        description: |
          Foreign key to dim_calendar.
        tests:
          - relationships:
              to: ref('dim_calendar')
              field: key
  - name: int_gtfs_schedule__daily_scheduled_service_index
    description: |
      An index listing date, feed, and `service_id` combinations for which service was scheduled (i.e.,
      the `service_id` is "in effect" and says that service occurred).
      Essentially, it takes `calendar` and `calendar_dates` for a given feed, takes the dates for which that
      feed was "in effect", and combines those into a long list of all service_ids that were in effect for a given
      date, then filters down to only those where service was actually scheduled on that date.
      For example, a row in this table with `feed_key = A`, `service_date = 2022-10-01`, `service_id = 1` indicates
      that:
      * Feed A was online on 2022-10-01
      * Service ID 1 covers the date 2022-10-01, i.e., if service ID 1 is defined in `calendar.txt`,
         `start_date <= 2022-10-01 <= end_date` and if service ID ` is defined in `calendar_dates.txt`,
         `2022-10-01` is listed as a `date` within that file.
      * The service indicator is `true` for 2022-10-01 (a Saturday). So, if this service was defined in `calendar.txt`,
         `saturday = 1` for `service_id = 1`, and there is no `exception_type = 2` in `calendar_dates.txt` for this service and date.
         If this service is defined exclusively in `calendar_dates.txt`, then `exception_type = 1` is listed for `2022-10-01` in that file.
      This table therefore excludes `service_id` values which were in effect (i.e., feed was online and date range of service
      overlaps with service date) but where service was not scheduled.
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - feed_key
            - service_date
            - service_id
    columns:
      - name: feed_key
        description: Foreign key for `dim_schedule_feeds`.
        tests:
          - not_null
      - name: calendar_key
        description: |
          Foreign key for `dim_calendar`. If null, the service for this date was
          defined exclusively via `calendar_dates`.
        tests:
          - relationships:
              to: ref('dim_calendar')
              field: key
      - name: calendar_dates_key
        description: |
          Foreign key for `dim_calendar_dates`. If null, the service for this date was
          defined exclusively via `calendar`.
        tests:
          - relationships:
              to: ref('dim_calendar_dates')
              field: key
      - name: service_date
        description: Date on which service was scheduled.
        tests:
          - not_null
      - name: service_id
        description: Service identifier from calendar and/or calendar_dates.
        tests:
          - not_null

  - name: int_gtfs_rt__unioned_parse_outcomes
    description: |
      A unioned combination of the parse outcomes for service alerts,
      vehicle positions, and trip updates data.
    columns:
      - name: dt
      - name: hour
        description: |
          Starting timestamp of hour in which data was downloaded, like `2022-10-31 23:00:00 UTC`.
      - name: name
      - name: url
      - name: feed_type,
      - name: _config_extract_ts,
      - name: schedule_url_for_validation,
      - name: parse_success
        description: |
          Boolean success indicator for whether this raw data was successfully included
          in the associated hourly aggregation.
      - name: parse_exception
        description: |
          If `parse_success` is false, the associated exception.
      - name: download_response_code
      - name: download_response_headers
      - name: step
      - name: base64_url
      - name: ts
      - name: last_modified_string
      - name: last_modified_timestamp
      - name: extract_ts

  - name: int_gtfs_schedule__incremental_stop_times
    description: |
      This table is an incremental, sparse history of GTFS schedule stop times data.
      It is a precursor to `dim_stop_times` with the goal of facilitating duplicate detection and missing key issues.
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - base64_url
            - ts
            - trip_id
            - stop_sequence
          severity: warn
    columns:
      - name: trip_id
        description: '{{ doc("gtfs_stop_times__trip_id") }}'
        tests:
        - not_null
      - name: arrival_time
        description: '{{ doc("gtfs_stop_times__arrival_time") }}'
      - name: departure_time
        description: '{{ doc("gtfs_stop_times__departure_time") }}'
      - name: stop_id
        description: '{{ doc("gtfs_stop_times__stop_id") }}'
        tests:
        - not_null:
            severity: warn
      - name: stop_sequence
        description: '{{ doc("gtfs_stop_times__stop_sequence") }}'
        tests:
        - not_null
      - name: stop_headsign
        description: '{{ doc("gtfs_stop_times__stop_headsign") }}'
      - name: pickup_type
        description: '{{ doc("gtfs_stop_times__pickup_type") }}'
      - name: drop_off_type
        description: '{{ doc("gtfs_stop_times__drop_off_type") }}'
      - name: continuous_pickup
        description: '{{ doc("gtfs_stop_times__continuous_pickup") }}'
      - name: continuous_drop_off
        description: '{{ doc("gtfs_stop_times__continuous_drop_off") }}'
      - name: shape_dist_traveled
        description: '{{ doc("gtfs_stop_times__shape_dist_traveled") }}'
      - name: timepoint
        description: '{{ doc("gtfs_stop_times__timepoint") }}'
      - name: _inserted_at
        description: |
          Timestamp for when the given row was inserted into the table.
          Primarily useful for debugging, not meaningful for general use.
  - name: int_gtfs_schedule__incremental_shapes
    description: |
      This table is an incremental, sparse history of GTFS schedule shapes data.
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - base64_url
            - ts
            - shape_id
            - shape_pt_sequence
    columns:
      - name: base64_url
      - name: ts
      - name: shape_id
        description: '{{ doc("gtfs_shapes__shape_id") }}'
        tests:
        - not_null
      - name: shape_pt_lat
        description: '{{ doc("gtfs_shapes__shape_pt_lat") }}'
        tests:
        - not_null
        meta:
          metabase.semantic_type: type/Latitude
          ckan.type: FLOAT
          ckan.length: 6
          ckan.precision: 3
      - name: shape_pt_lon
        description: '{{ doc("gtfs_shapes__shape_pt_lon") }}'
        tests:
        - not_null
        meta:
          metabase.semantic_type: type/Longitude
          ckan.type: FLOAT
          ckan.length: 7
          ckan.precision: 3
      - name: shape_pt_sequence
        description: '{{ doc("gtfs_shapes__shape_pt_sequence") }}'
        tests:
        - not_null
      - name: shape_dist_traveled
        description: '{{ doc("gtfs_shapes__shape_dist_traveled") }}'
  - name: int_gtfs_schedule__keyed_parse_outcomes
    description: |
      All GTFS schedule file parse outcomes, with `feed_key` identifier joined
      on to facilitate downstream use.
    tests:
      - dbt_utils.equal_rowcount:
          compare_model: ref('stg_gtfs_schedule__file_parse_outcomes')
    columns:
      - name: feed_key
        description: |
          Foreign key to `dim_schedule_feeds`.
          Because `dim_schedule_feeds` only includes unique feed versions, this will not be populated
          for downloads where no data changed relative to what had already been ingested.
        tests:
          - relationships:
              to: ref('dim_schedule_feeds')
              field: key
      - name: parse_success
        description: '{{ doc("column_schedule_parse_success") }}'
      - name: parse_exception
        description: '{{ doc("column_schedule_parse_exception") }}'
      - name: filename
        description: '{{ doc("column_schedule_parse_filename") }}'
      - name: _config_extract_ts
      - name: feed_name
        description: '{{ doc("column_schedule_parse_feed_name") }}'
      - name: feed_url
        description: '{{ doc("column_schedule_parse_feed_url") }}'
      - name: original_filename
        description: '{{ doc("column_schedule_parse_original_filename") }}'
      - name: gtfs_filename
        description: '{{ doc("column_schedule_parse_gtfs_filename") }}'
      - name: dt
      - name: ts
      - name: base64_url
  - name: int_gtfs_schedule__stop_times_grouped
    description: |
      Stop times grouped by trip_id with trip aggregation calculations.
    columns:
      - name: feed_key
      - name: trip_id
      - name: n_stops
      - name: n_stop_times
      - name: trip_first_departure_sec
      - name: trip_last_arrival_sec
      - name: service_hours
      - name: contains_warning_duplicate_stop_times_primary_key
        description: |
          Rows with `true` in this column indicate that the columns in this table that are aggregated from
          stop times data (`n_stops`, `n_stop_times`,  `trip_first_departure_ts`, `trip_last_arrival_ts`,
          and `service_hours`) contain at least one row that had a duplicate primary key in the source stop times data.

          I.e., at least one row being aggregated had a `trip_id` / `stop_sequence` pair that was not unique
          in the input data. This indicates that data quality issues were present in the stop times data
          that is being summarized here, and counts may be inflated due to multiple rows with identical identifiers.
      - name: contains_warning_missing_foreign_key_stop_id
        description: |
          Rows with `true` in this column indicate that the columns in this table that are aggregated from
          stop times data (`n_stops`, `n_stop_times`,  `trip_first_departure_ts`, `trip_last_arrival_ts`,
          and `service_hours`) contain at least one row that had a missing `stop_id` foreign key in the source stops data.

          I.e., at least one row being aggregated had a `stop_id` foreign key that was missing
          in the input data. This indicates that data quality issues were present in the stop times data
          that is being summarized here, and the count of distinct `stop_id`s at the trip level may be incorrect.
  - name: int_gtfs_rt__distinct_download_configs
    description: |
      Distinct `dt`, `_config_extract_ts` pairs indicating
      the configuration versions that were in effect for a given date of RT data.
      This allows us to reconstruct what URLs were in our configuration list for each date.
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - dt
            - _config_extract_ts
    columns:
      - name: dt
        description: Date that RT data was extracted.
      - name: _config_extract_ts
        description: Timestamp that download config file was extracted.
  - name: int_gtfs_rt__daily_url_index
    description: |
      All RT URLs that were present at least once for each given day.
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - dt
            - base64_url
    columns:
      - name: dt
        description: |
          Date that this URL was present in an active download configuration,
          indicating that downloads should have been attempted for this URL
          on this date.
        tests:
          - not_null
      - name: string_url
        description: |
          URL in human-readable format.
        tests:
          - not_null
      - name: base64_url
        description: |
          Base64 encoded URL, can be used to join with other tables
          containing data from this feed.
        tests:
          - not_null
      - name: type
        description: |
          RT data type: "GTFS Alerts", "GTFS VehiclePositions", "GTFS TripUpdates".
        tests:
          - not_null
      - name: data_quality_pipeline
  - name: int_gtfs_schedule__all_scheduled_service
    description: |
      **Use with caution: This table lists service for all feeds, regardless of whether the given feed
      was actually "active" on the given day. To see only service for active feeds,
      consult `int_gtfs_schedule__daily_scheduled_service_index`.**
      Lists date, feed, and `service_id` combinations for which service was scheduled (i.e.,
      the `service_id` is "in effect" and says that service occurred).
      Essentially, it takes `calendar` and `calendar_dates` for a given feed, takes the dates for which that
      feed was "in effect", and combines those into a long list of all service_ids that were in effect for a given
      date, then filters down to only those where service was actually scheduled on that date.
      For example, a row in this table with `feed_key = A`, `service_date = 2022-10-01`, `service_id = 1` indicates
      that:
      * Service ID 1 covers the date 2022-10-01, i.e., if service ID 1 is defined in `calendar.txt`,
         `start_date <= 2022-10-01 <= end_date` and if service ID 1 is defined in `calendar_dates.txt`,
         `2022-10-01` is listed as a `date` within that file.
      * The service indicator is `true` for 2022-10-01 (a Saturday). So, if this service was defined in `calendar.txt`,
         `saturday = 1` for `service_id = 1`, and there is no `exception_type = 2` in `calendar_dates.txt` for this service and date.
         If this service is defined exclusively in `calendar_dates.txt`, then `exception_type = 1` is listed for `2022-10-01` in that file.
    columns:
      - *schedule_key
      - name: feed_key
        description: Foreign key for `dim_schedule_feeds`.
        tests:
          - not_null
      - name: calendar_key
        description: |
          Foreign key for `dim_calendar`. If null, the service for this date was
          defined exclusively via `calendar_dates`.
        tests:
          - relationships:
              to: ref('dim_calendar')
              field: key
      - name: calendar_dates_key
        description: |
          Foreign key for `dim_calendar_dates`. If null, the service for this date was
          defined exclusively via `calendar`.
        tests:
          - relationships:
              to: ref('dim_calendar_dates')
              field: key
      - name: service_date
        description: Date on which service was scheduled.
        tests:
          - not_null
      - name: service_id
        description: Service identifier from calendar and/or calendar_dates.
        tests:
          - not_null
  - name: int_gtfs_schedule__incremental_stops
    description: |
      This table is an incremental, sparse history of GTFS schedule stops data. It is a precursor to `dim_stops`
      with the goal of facilitating duplicate detection. It also drops entire duplicate rows (rows where all
      fields are identical.)
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns:
            - base64_url
            - ts
            - stop_id
          severity: warn
    columns:
      - name: base64_url
      - name: ts
      - name: stop_id
        description: '{{ doc("gtfs_stops__stop_id") }}'
      - name: stop_code
        description: '{{ doc("gtfs_stops__stop_code") }}'
      - name: stop_name
        description: '{{ doc("gtfs_stops__stop_name") }}'
      - name: tts_stop_name
        description: '{{ doc("gtfs_stops__tts_stop_name") }}'
      - name: stop_desc
        description: '{{ doc("gtfs_stops__stop_desc") }}'
      - name: stop_lat
        description: '{{ doc("gtfs_stops__stop_lat") }}'
        meta:
          metabase.semantic_type: type/Latitude
          ckan.type: FLOAT
          ckan.length: 6
          ckan.precision: 3
      - name: stop_lon
        description: '{{ doc("gtfs_stops__stop_lon") }}'
        meta:
          metabase.semantic_type: type/Longitude
          ckan.type: FLOAT
          ckan.length: 7
          ckan.precision: 3
      - name: zone_id
        description: '{{ doc("gtfs_stops__zone_id") }}'
      - name: stop_url
        description: '{{ doc("gtfs_stops__stop_url") }}'
      - name: location_type
        description: '{{ doc("gtfs_stops__location_type") }}'
      - name: parent_station
        description: '{{ doc("gtfs_stops__parent_station") }}'
      - name: stop_timezone
        description: '{{ doc("gtfs_stops__stop_timezone") }}'
      - name: wheelchair_boarding
        description: '{{ doc("gtfs_stops__wheelchair_boarding") }}'
      - name: level_id
        description: '{{ doc("gtfs_stops__level_id") }}'
      - name: platform_code
        description: '{{ doc("gtfs_stops__platform_code") }}'
      - name: _inserted_at
        description: |
          Timestamp for when the given row was inserted into the table.
          Primarily useful for debugging, not meaningful for general use.

  - name: int_gtfs_rt__trip_updates_no_stop_times
    description: |
      This is really intended to be a temporary table until
      this work is done at the Airflow level, most likely.
      Incrementally materialized trip updates data for use
      downstream to minimize amount of raw data read from
      GCS.
