operator: 'operators.PodOperator'
name: 'dbt-test'
image: 'ghcr.io/cal-itp/data-infra/warehouse:{{ image_tag() }}'

cmds:
  - python3
arguments:
  - '/app/scripts/run_and_upload.py'
  - 'run'
  - '--no-dbt-seed'
  - '--no-dbt-run'
  - '--dbt-test'
  - '--select'
  - "{{ dag_run.conf.get('dbt_select_statement', '') }}"

dependencies:
  - dbt_run_and_upload_artifacts
trigger_rule: all_done

is_delete_operator_pod: true
get_logs: true
is_gke: true
pod_location: us-west1
cluster_name: data-infra-apps
namespace: airflow-jobs

env_vars:
  AIRFLOW_ENV: "{{ env_var('AIRFLOW_ENV') }}"
  CALITP_BUCKET__DBT_ARTIFACTS: "{{ env_var('CALITP_BUCKET__DBT_ARTIFACTS') }}"
  BIGQUERY_KEYFILE_LOCATION: /secrets/jobs-data/service_account.json
  DBT_PROJECT_DIR: /app
  DBT_PROFILE_DIR: /app
  DBT_TARGET: "{{ env_var('DBT_TARGET') }}"
  SENTRY_DSN: "{{ env_var('SENTRY_DSN') }}"
  SENTRY_ENVIRONMENT: "{{ env_var('SENTRY_ENVIRONMENT') }}"

secrets:
  - deploy_type: volume
    deploy_target: /secrets/jobs-data/
    secret: jobs-data
    key: service-account.json

k8s_resources:
  request_memory: 2.0Gi
  request_cpu: 1

tolerations:
  - key: pod-role
    operator: Equal
    value: computetask
    effect: NoSchedule

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: pod-role
          operator: In
          values:
          - computetask
