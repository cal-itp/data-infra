operator: 'operators.PodOperator'
name: 'dbt-test'
image: 'ghcr.io/cal-itp/data-infra/warehouse:latest'

cmds:
  - python3
arguments:
  - '/app/scripts/run_and_upload.py'
  - '--no-dbt-run'
  - '--no-dbt-test'
  - '--dbt-source-freshness'

dependencies:
  - dbt_run_and_upload_artifacts

is_delete_operator_pod: true
get_logs: true
is_gke: true
pod_location: us-west1
cluster_name: data-infra-apps
namespace: airflow-jobs

env_vars:
  BIGQUERY_KEYFILE_LOCATION: /secrets/jobs-data/service_account.json
  DBT_PROJECT_DIR: /app
  DBT_PROFILE_DIR: /app
  DBT_TARGET: prod_service_account
secrets:
  - deploy_type: volume
    deploy_target: /secrets/jobs-data/
    secret: jobs-data
    key: service-account.json

tolerations:
  - key: pod-role
    operator: Equal
    value: computetask
    effect: NoSchedule
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: pod-role
          operator: In
          values:
          - computetask
