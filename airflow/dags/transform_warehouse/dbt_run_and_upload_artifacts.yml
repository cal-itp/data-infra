operator: 'operators.PodOperator'
name: 'dbt-run-and-upload-artifacts'
image: 'ghcr.io/cal-itp/data-infra/warehouse:latest'

cmds:
  - python3
arguments:
  - '/app/scripts/run_and_upload.py'
  - '--no-dbt-test'
  - '--dbt-docs'
  - '--save-artifacts'
  - '--deploy-docs'
  - '--sync-metabase'

is_delete_operator_pod: true
get_logs: true
is_gke: true
pod_location: us-west1
cluster_name: data-infra-apps
namespace: airflow-jobs

env_vars:
  BIGQUERY_KEYFILE_LOCATION: /secrets/jobs-data/service_account.json
  DBT_PROJECT_DIR: /app
  DBT_PROFILE_DIR: /app
  DBT_TARGET: prod_service_account
secrets:
  - deploy_type: volume
    deploy_target: /secrets/jobs-data/
    secret: jobs-data
    key: service-account.json
  - deploy_type: env
    deploy_target: METABASE_USER
    secret: jobs-data
    key: metabase-user
  - deploy_type: env
    deploy_target: METABASE_PASSWORD
    secret: jobs-data
    key: metabase-password
  - deploy_type: env
    deploy_target: NETLIFY_AUTH_TOKEN
    secret: jobs-data
    key: netlify-auth-token

tolerations:
  - key: pod-role
    operator: Equal
    value: computetask
    effect: NoSchedule
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: pod-role
          operator: In
          values:
          - computetask
