(developing-dbt-models)=
# Developing models in dbt

Information related to contributing to the [Cal-ITP dbt project](https://github.com/cal-itp/data-infra/tree/main/warehouse).

## Resources

* If you have questions specific to our project or you encounter any issues when developing, please reach out in the `#data-warehouse-devs` or `#data-office-hours` Cal-ITP Slack channels.
* For Cal-ITP-specific data warehouse documentation, including high-level concepts and naming conventions, see [our Cal-ITP dbt documentation site](https://dbt-docs.calitp.org/#!/overview). This documentation is automatically generated by dbt, and incorporates the table- and column-level documentation that developers enter in YAML files in the dbt project.
* For general dbt concepts (for example, dbt [Jinja](https://docs.getdbt.com/guides/advanced/using-jinja) or [tests](https://docs.getdbt.com/docs/build/tests)), see the [general dbt documentation site](https://docs.getdbt.com/docs/introduction)
* For general SQL or BigQuery concepts (for example, [tables](https://cloud.google.com/bigquery/docs/tables-intro), [views](https://cloud.google.com/bigquery/docs/views-intro), or [window functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/window-function-calls)), see the [BigQuery docs site](https://cloud.google.com/bigquery/docs).

## How to contribute to the dbt project

### Getting started

To get set up to contribute to the dbt project via JupyterHub, follow [the README in the data-infra repo warehouse folder](https://github.com/cal-itp/data-infra/blob/main/warehouse/README.md#setting-up-the-project-in-your-jupyterhub-personal-server). If you hit any trouble with setup, let folks know in the `#data-warehouse-devs` or `#data-office-hours` channels in the Cal-ITP Slack.

We also recommend that everyone who does dbt development joins the `#data-warehouse-devs` channel in the Cal-ITP Slack workspace to ask questions, collaborate, and build shared knowledge.

### Developer workflow

```{admonition} See next section for modeling considerations
This section describes the high-level mechanics/process of the developer workflow to edit the dbt project.
**Please read the next section for things you should consider from the data modeling perspective.**
```

To test your work while developing dbt models, you can edit the `.sql` files for your models, save your changes, and then [run the model from the command line](https://github.com/cal-itp/data-infra/tree/main/warehouse#dbt-commands) to execute the SQL you updated.

To inspect tables as you are working, the fastest method is usually to run some manual test queries or "preview" the tables in the [BigQuery user interface](https://console.cloud.google.com/bigquery?project=cal-itp-data-infra-staging). You can also use something like [`pandas.read_gbq`](https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html) to perform example queries in a notebook.

When you run dbt commands locally on JupyterHub, your models will be created in the `cal-itp-data-infra-staging.<your name>_<dbt folder name, like mart_gtfs>` BigQuery dataset. Note that this is in the `cal-itp-data-infra-staging` Google Cloud Platform project, *not* the production `cal-itp-data-infra` project.

Once your models are working the way you want, please make sure to update the associated YAML files (there will generally be one or two YAML files per folder with model tests, documentation, and additional configuration.) Especially if you created a brand-new model, you will want to add tests for things like unique, non-null primary keys and valid foreign keys. The YAML is also where table- and column-level documentation is populated. [Here is an example YAML file from our project](https://github.com/cal-itp/data-infra/blob/main/warehouse/models/mart/gtfs/_mart_gtfs_dims.yml), and [here is an example PR that created a new mart table with accompanying documentation](https://github.com/cal-itp/data-infra/pull/2097).

Because the warehouse is collectively maintained and changes can affect a variety of users, please open PRs against `main` when work is ready to merge and keep an eye out for comments and questions from reviewers, who might require tweaks before merging. See CONTRIBUTING.md in the repo for more information on GitHub practices.

## Modeling considerations

When developing or updating dbt models, there are some considerations which may differ from considerations for a notebook-based analysis. These can be thought of as a checklist or decision tree of questions that you should run through whenever you are editing or creating a dbt model. Longer explanations of each item are described below.

```{mermaid}
flowchart TD

workflow_type[Are you fixing a bug or creating something new?]
identify_bug[<a href='developing_dbt_models#identify-bug'>Identify the cause of your bug.</a>]
identify_bug[<a href='developing_dbt_models#fix-bug'>Fix the bug.</a>]
tool_choice[<a href='developing_dbt_models#tool-choice'>Should this be a dbt model or a different type of analysis, for example a Jupyter notebook or a dashboard?</a>]
not_dbt[Use a notebook or dashboard for your analysis.]
grain[<a href='developing_dbt_models#tool-choice'>What is the grain/row definition of your target model?<br>Is there already a model with this grain?</a>]
add_column[Add a column to the existing model.]
new_model[Create a new model with your desired grain.]
test_changes[Test your changes in the staging environment.<br>Are the values what you expect?<br>Are there null values?<br>Did you change the number of rows in the model?<br>Did you substantially change the size in bytes of the model?<br>etc.]

workflow_type -- fixing a bug --> identify_bug
workflow_type -- creating something new --> tool_choice
tool_choice -- dbt model--> grain
tool_choice -- not dbt --> not_dbt
grain -- same grain as existing model --> add_column
grain -- new grain --> new_model
add_column --> test_changes
new_model --> test_changes
fix_bug --> test_changes
```

(identify-bug)=
### Identify the cause of your bug.

Usually, a bug is caused by either:
* New data issues. For example, an agency may be doing something new in their GTFS data that we didn't expect and this may have broken one of our models.
* SQL bugs. Sometimes we may have written SQL incorrectly (for example, used the wrong kind of join.)

How to investigate the bug depends on how the bug was noticed.

If there was a failing dbt test, you can `dbt compile` locally to compile the project SQL. You can then find the SQL for the failing test (follow the [dbt testing FAQ under "one of my tests failed, how can I debug it?"](https://docs.getdbt.com/docs/build/tests#faqs) to find the compiled test SQL). Run that SQL in BigQuery to see the rows that are failing.

If you noticed an issue that wasn't caused by a failing test, you can start with the model that you noticed the problem in.

In either case, you may need to consider upstream models. To identify your model's parents, you can look at the [dbt docs website](https://dbt-docs.calitp.org/#!/overview) page for your model. [See the dbt docs](https://docs.getdbt.com/docs/collaborate/documentation#navigating-the-documentation-site) for how to look at the model's lineage. You can modify the model selector in the bottom middle to just `+<your model name>` to only see the model's parents. You can also run `poetry run dbt ls -s +<your model> --resource-type model` to see a model's parents just on the command line. Try to figure out where the root cause of the problem is occurring. This may involve running ad-hoc SQL queries to inspect the models involved.

(fix-bug)=
### Fix the bug.

Once you understand the issue, you can attempt to fix it. This might involve changing model SQL, changing tests to reflect a new understanding, or something else.

Here are a few example `data-infra` PRs that fixed past bugs:
- [#2076](https://github.com/cal-itp/data-infra/pull/2076) fixed two bugs: There was a hardcoded incorrect value in our SQL that was causing Sundays to not appear in our scheduled service index (SQL syntax bug), and there was a bug in how we were handling the relationship between `calendar_dates` and `calendar` (GTFS logic bug).
- [#2623](https://github.com/cal-itp/data-infra/pull/2623) fixed bugs caused by unexpected calendar data from a producer.

(tool_choice)=
### Should this be a dbt model?

Changes to dbt models are likely to be appropriate, and often beneficial over other approaches, when one or more of the following is true:
* There is a consistent or ongoing need for this data. dbt can ensure that transformations are performed consistently at scale, every day.
* The data is big. Doing transformations in BigQuery can be more performant than doing them in notebooks or any workflow where the large data must be loaded into local memory.
* We want to use the same model across multiple domains or tools. The BigQuery data warehouse is the easiest way to provide consistent data throughout the Cal-ITP data ecosystem (in JupyterHub, Metabase, open data publishing, the reports site, etc.)

dbt models may not be appropriate when:
* You are doing exploratory data analysis, especially on inconsistently-constructed data. It will almost always be faster to do initial exploration of data via Jupyter/Python than in SQL.
* You want to apply a simple transformation (for example, a grouped summary or filter, or a join) to answer a specific question. In this case, it may be more appropriate to simply create a Metabase dashboard with the desired transformations.

(model-grain)=
### What is the grain/row definition of your target model? Is there already a model with this grain?

*Grain* means "what does a row represent". For example: Do you want one row per route per day? One row per fare transaction? One row per organization per month?

This concept of grain can be one of the biggest differences between notebook-based analysis and warehouse analytics engineering. In notebooks, you may be making a lot of transformations and saving each step out as its own dataframe, and you may use functions for reusable transformation steps. In warehouse development, we want to be focused on making reusable models, where the data itself is the common building block across analyses. That often means trying to make only one table in the warehouse for each grain, regardless of how many different types of analysis it might be used for.

``` {annotation} Example: fct_scheduled_trips
Consider [`fct_scheduled_trips`](https://dbt-docs.calitp.org/#!/model/model.calitp_warehouse.fct_scheduled_trips). This is our core trip-level table. Every scheduled trip should have a row in this model and attributes that you might want from that trip should be present for easy access. As a result, this table has a lot of columns, because when we need new information about trips, we add it here.  For example, when we wanted to fix time zone handling for trips, we [added those columns](https://github.com/cal-itp/data-infra/pull/2457) instead of creating a new model.
```

If there is already a model with the grain you are targeting, you should almost always add new columns to that existing model rather than making a new model with the same grain.

To figure out if there is a model with your desired grain, you can [search the dbt docs](https://dbt-docs.calitp.org/#!/overview) for relevant terms. For example, if you want a table of routes, you can search "routes" to see what models already exist. You can also explore the dependency tree for a related table (like `dim_routes`) to see if you can find a table that looks like it has the right grain. You can also see [our dbt docs homepage](https://dbt-docs.calitp.org/#!/overview) for a discussion of table naming conventions to interpret dimension, fact, and bridge tables.

### Should I create a new model or update an existing model?



If you determine a dbt update is appropriate, you must decide whether a given data need is best met by creating a new dbt model or updating an existing model.

The main consideration should be: **Is there already a model with the grain that my new model would be?**

For example, say you want to create a model with the count of scheduled stop events by route per month. Your new model would have one row per route per month. Say that there is already a model that lists the count of scheduled trips per route per month. That model also has one row per route per month. So, you should add a column with a stop event count to that existing model, rather than making a brand new model.


### Materializations, performance, and cost

Because the dbt project is run and built every day, we want to be mindful of cost and be efficient in how we build and structure our models. The [dbt docs materializations page](https://docs.getdbt.com/docs/build/materializations#overview) provides a good overview of different materialization options and associated considerations.

When developing a new model, or updating an existing model, it is helpful to keep an eye on the number of bytes billed to build the model (this information is printed in the terminal output from dbt.) As a rule of thumb in our project, models that take more than 100 GB to build should probably be optimized a bit more, potentially by being made [incremental](https://docs.getdbt.com/docs/build/materializations#incremental).

Performance is one of the hardest things to manage when you are new to developing in SQL, so please don't hesitate to ask questions (the `#data-warehouse-devs` or `#data-office-hours` Cal-ITP Slack channels are good places to ask) as you get used to the options.

## Helpful talks and presentations

### dbt at Cal-ITP introduction

In 2022, Laurie [gave a lunch and learn](https://cal-itp.slack.com/archives/C02NJRV9QUB/p1651082191730229?thread_ts=1650648699.809809&cid=C02NJRV9QUB) about why we are using dbt at Cal-ITP.

### Coalesce

Some folks from Data Services attended Coalesce (dbt's conference) in 2022 and thought the following talks may be of interest:

* [The accidental analytics engineer by Michael Chow](https://www.youtube.com/watch?v=EYdb1x1cO9U&list=PL0QYlrC86xQlj9UDGiEwhXQuSjuSyPJHl&index=66) - this talk outlines some differences Michael has experienced between R/tidyverse and dbt/MDS (modern data stack) approaches to working with data
* [dbt and MDS in small-batch academic research](https://www.youtube.com/watch?v=0SDp1yTK2zc&list=PL0QYlrC86xQlj9UDGiEwhXQuSjuSyPJHl&index=112) - this talk outlines some benefits this researcher found to using dbt in an academic context; note that he uses DuckDB (instead of BigQuery)
