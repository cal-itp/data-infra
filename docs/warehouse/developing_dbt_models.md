(developing-dbt-models)=
# Developing models in dbt

Information related to contributing to the [Cal-ITP dbt project](https://github.com/cal-itp/data-infra/tree/main/warehouse).

## Resources

* If you have questions specific to our project or you encounter any issues when developing, please reach out in the `#data-warehouse-devs` or `#data-office-hours` Cal-ITP Slack channels.
* For Cal-ITP-specific data warehouse documentation, including high-level concepts and naming conventions, see [our Cal-ITP dbt documentation site](https://dbt-docs.calitp.org/#!/overview). This documentation is automatically generated by dbt, and incorporates the table- and column-level documentation that developers enter in YAML files in the dbt project.
* For general dbt concepts (for example, dbt [Jinja](https://docs.getdbt.com/guides/advanced/using-jinja) or [tests](https://docs.getdbt.com/docs/build/tests)), see the [general dbt documentation site](https://docs.getdbt.com/docs/introduction)
* For general SQL or BigQuery concepts (for example, [tables](https://cloud.google.com/bigquery/docs/tables-intro), [views](https://cloud.google.com/bigquery/docs/views-intro), or [window functions](https://cloud.google.com/bigquery/docs/reference/standard-sql/window-function-calls)), see the [BigQuery docs site](https://cloud.google.com/bigquery/docs).

## How to contribute to the dbt project

### Getting started

To get set up to contribute to the dbt project via JupyterHub, follow [the README in the data-infra repo warehouse folder](https://github.com/cal-itp/data-infra/blob/main/warehouse/README.md#setting-up-the-project-in-your-jupyterhub-personal-server). If you hit any trouble with setup, let folks know in the #data-warehouse-devs or #data-office-hours channel in the Cal-ITP Slack.

We also recommend that everyone who does dbt development joins the `#data-warehouse-devs` channel in the Cal-ITP Slack workspace to ask questions, collaborate, and build shared knowledge.

### Developer workflow

To test your work while developing dbt models, you can edit the `.sql` files for your models, save your changes, and then [run the model from the command line](https://github.com/cal-itp/data-infra/tree/main/warehouse#dbt-commands) to execute the SQL you updated.

To inspect tables as you are working, the fastest method is usually to run some manual test queries or "preview" the tables in the [BigQuery user interface](https://console.cloud.google.com/bigquery?project=cal-itp-data-infra-staging). You can also use something like [`pandas.read_gbq`](https://pandas.pydata.org/docs/reference/api/pandas.read_gbq.html) to perform example queries in a notebook.

When you run dbt commands locally on JupyterHub, your models will be created in the `cal-itp-data-infra-staging.<your name>_<dbt folder name, like mart_gtfs>` BigQuery dataset. Note that this is in the `cal-itp-data-infra-staging` Google Cloud Platform project, *not* the production `cal-itp-data-infra` project.

Once your models are working the way you want, please make sure to update the associated YAML files (there will generally be one or two YAML files per folder with model tests, documentation, and additional configuration.) Especially if you created a brand-new model, you will want to add tests for things like unique, non-null primary keys and valid foreign keys. The YAML is also where table- and column-level documentation is populated. [Here is an example YAML file from our project](https://github.com/cal-itp/data-infra/blob/main/warehouse/models/mart/gtfs/_mart_gtfs_dims.yml), and [here is an example PR that created a new mart table with accompanying documentation](https://github.com/cal-itp/data-infra/pull/2097).

Because the warehouse is collectively maintained and changes can affect a variety of users, please open PRs against `main` when work is ready to merge and keep an eye out for comments and questions from reviewers, who might require tweaks before merging. See CONTRIBUTING.md in the repo for more information on GitHub practices.)

## Modeling considerations

When developing dbt models, there are some considerations which may differ from considerations for a notebook-based analysis.

### When to develop or update a model

One key question to ask is whether a given data need is best met by a new dbt model or updates to an existing model vs. some other tool or process.

Changes to dbt models are likely to be appropriate, and often beneficial over other approaches, when one or more of the following is true:
* There is a consistent or ongoing need for the same transformations. dbt can ensure that transformations are performed consistently at scale, every day.
* Transformations are needed on large data. Doing transformations in BigQuery can be more performant than doing them in notebooks or any workflow where the large data must be loaded into local memory.
* We want to use the same model across multiple domains or tools. The BigQuery data warehouse is the easiest way to provide consistent data throughout the Cal-ITP data ecosystem (in JupyterHub, Metabase, open data publishing, the reports site, etc.)

dbt model updates may not be appropriate when:
* There is insufficient support in dbt or BigQuery for the necessary tooling. The biggest current example is geospatial work; once we have [Python models in the dbt project](https://github.com/cal-itp/data-infra/issues/2359), there will be fewer limitations.
* You are doing exploratory data analysis, especially on inconsistently-constructed data. It will almost always be faster to do initial exploration of data via Jupyter/Python than in SQL.

### Materializations, performance, and cost

Because the dbt project is run and built every day, we want to be mindful of cost and be efficient in how we build and structure our models. The [dbt docs materializations page](https://docs.getdbt.com/docs/build/materializations#overview) provides a good overview of different materialization options and associated considerations.

When developing a new model, or updating an existing model, it is helpful to keep an eye on the number of bytes billed to build the model (this information is printed in the terminal output from dbt.) As a rule of thumb in our project, models that take more than 100 GB to build should probably be optimized a bit more, potentially by being made [incremental](https://docs.getdbt.com/docs/build/materializations#incremental).

Performance is one of the hardest things to manage when you are new to developing in SQL, so please don't hesitate to ask questions (the `#data-warehouse-devs` or `#data-office-hours` Cal-ITP Slack channels are good places to ask) as you get used to the options.

## Helpful talks and presentations

### dbt at Cal-ITP introduction

In 2022, Laurie [gave a lunch and learn](https://cal-itp.slack.com/archives/C02NJRV9QUB/p1651082191730229?thread_ts=1650648699.809809&cid=C02NJRV9QUB) about why we are using dbt at Cal-ITP.

### Coalesce

Some folks from Data Services attended Coalesce (dbt's conference) in 2022 and thought the following talks may be of interest:

* [The accidental analytics engineer by Michael Chow](https://www.youtube.com/watch?v=EYdb1x1cO9U&list=PL0QYlrC86xQlj9UDGiEwhXQuSjuSyPJHl&index=66) - this talk outlines some differences Michael has experienced between R/tidyverse and dbt/MDS (modern data stack) approaches to working with data
* [dbt and MDS in small-batch academic research](https://www.youtube.com/watch?v=0SDp1yTK2zc&list=PL0QYlrC86xQlj9UDGiEwhXQuSjuSyPJHl&index=112) - this talk outlines some benefits this researcher found to using dbt in an academic context; note that he uses DuckDB (instead of BigQuery)
