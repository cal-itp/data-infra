# Production Maintenance

_Edited March 2022_

## Re-running or clearing DAGs

Sometimes DAGs will fail in production. Below are considerations to take into account when re-running or clearing DAGs to address failures.

Summary of key considerations:
* **Safe after 24h** - some DAGs scrape data and then label that data based on execution date (i.e., if they are run more than 24h after they were scheduled, they will pull data from one day and label it as having come from a prior day.) Long-term we will work to break this pattern, but until those changes are made, **these DAGs must not be re-run 24 hours after originally scheduled (including backfills)**.
    * If **â›” no**: **DO NOT RUN OR CLEAR IF IT IS MORE THAN 24h AFTER ORIGINALLY SCHEDULED EXECUTION; this consideration trumps all other considerations in the other attributes**
    * If yes: Safe to re-run any time
* **`depends_on_past`** - Some DAGs depend on a prior day's run having completed successfully, denoted by the `depends_on_past` flag. This means that if the DAG fails for multiple days, each day must be re-run in order.
    * If no, this will run even if prior days have failed
    * If **ðŸ“† yes**, this will wait for prior days to succeed before current day can succeed
* **All of history** - Some DAGs contain only operations that cover all of history, even when only for one day (i.e., none of their tasks do anything that is specific to their `execution_date`.) So if these DAGs fail for multiple days in a row, only the latest day needs to be re-run to refresh all the data.
    * If **ðŸ”‚ no**, must re-run this DAG for each day that failed to ensure that all data is captured
    * If yes, can re-run only the most recent date
* **Depends on** - The information in the table below is a high-level summary indicating general DAG execution order and explicit inter-DAG dependencies; there may be additional dependencies for individual tasks within a DAG, and there may be cases where a DAG task depends on **data** generated by another DAG task but the task-task dependency is not formally specified.
    * If N/A, this DAG is the bottom of the dependency ladder and can run independently.
    * If populated, this DAG has upstream dependencies that must complete before it will run


DAGs are listed in alphabetical order, as they appear in the Airflow UI.

| DAG | Safe after 24h | `depends_ on_past` | All of history | Depends on | Notes |
| --- | --- | --- | --- | --- | --- |
`airtable_loader` | **â›” No (all tasks)** | No | **ðŸ”‚ No** | N/A | |
`airtable_views` | Yes | No | Yes* | `airtable_loader` | Latest-only data |
`gtfs_downloader` | **â›” No (`generate_provider_list` and `download_data` tasks specifically)** | No | **ðŸ”‚ No** | N/A | Tasks downstream of `download_data` can safely be rerun after 24 hours |
`gtfs_loader` | Yes | No | **ðŸ”‚ No** | `gtfs_downloader`* | Technically also depends on `gtfs_schedule_history`, not usually an issue |
`gtfs_schedule` | Yes | No | Yes* | `gtfs_views_staging` | Latest-only data (but depends on `gtfs_views_staging` for data cleaning) |
`gtfs_schedule_history` | N/A | N/A | N/A | N/A | Once-only (defines external tables); does not generally need to be re-run |
`gtfs_schedule_history2` | Yes | **ðŸ“† Yes** | **ðŸ”‚ No** | `gtfs_loader` | |
`gtfs_views` | Yes | No | Yes | `gtfs_views_staging` | |
`gtfs_views_staging` | Yes | No | Yes | `gtfs_schedule_history2` | |
`payments_loader` | Yes | No | Yes | N/A | |
`payments_views` | Yes | No | Yes | `payments_views_staging`| |
`payments_views_staging` | Yes | No | Yes | `payments_loader` | |
`rt_loader` | Yes | No | **ðŸ”‚ No** | `gtfs_loader` | |
`rt_loader_files` | Yes | No | **ðŸ”‚ No** | N/A | |
`rt_timestamp_fix` | N/A | N/A | N/A | N/A | DAG is deprecated but still appears in Airflow UI |
`rt_views` | Yes | No | Yes | `rt_loader`, `gtfs_views` | |
`sandbox` | N/A | N/A | N/A | N/A | Testing only; does not need to be re-run |

## Task-level considerations

Some tasks have unique considerations, beyond the requirements of their overall DAG.

* **`PodOperators`**: When restarting a failed `PodOperator` run, check the logs before restarting. If the logs show any indication that the prior run's pod was not killed (for example, if the logs cut off abruptly without showing an explicit task failure), you should check that the pod associated with the failed run task has in fact been killed before clearing or restarting the Airflow task.

## Backfilling from the command line

From time-to-time some DAGs may need to be re-ran in order to populate new data.

More will be added later, but whenever backfilling of DAG tasks need to happen, the following command can be ran, subject to the considerations outlined above:

```shell
gcloud composer environments run calitp-airflow-prod --location=us-west2 backfill -- --start_date 2021-04-18 --end_date 2021-11-03 -x --reset_dagruns -y -t "gtfs_schedule_history_load" -i gtfs_loader
```
