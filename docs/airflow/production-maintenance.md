# Production Maintenance

_Last edited: May 5, 2022_

## Re-running or clearing DAGs

Sometimes DAGs will fail in production. Below are considerations to take into account when re-running or clearing DAGs to address failures.

Summary of key considerations:
* **Safe after 24h** - some DAGs scrape data and then label that data based on execution date (i.e., if they are run more than 24h after they were scheduled, they will pull data from one day and label it as having come from a prior day.) Long-term we will work to break this pattern, but until those changes are made, **these DAGs must not be re-run 24 hours after originally scheduled (including backfills)**.
    * If **â›” no**: **DO NOT RUN OR CLEAR IF IT IS MORE THAN 24h AFTER ORIGINALLY SCHEDULED EXECUTION; this consideration trumps all other considerations in the other attributes**
    * If yes: Safe to re-run any time
* **`depends_on_past`** - Some DAGs depend on a prior day's run having completed successfully, denoted by the `depends_on_past` flag. This means that if the DAG fails for multiple days, each day must be re-run in order.
    * If no, this will run even if prior days have failed
    * If **ðŸ“† yes**, this will wait for prior days to succeed before current day can succeed
* **All of history** - Some DAGs contain only operations that cover all of history, even when only for one day (i.e., none of their tasks do anything that is specific to their `execution_date`.) So if these DAGs fail for multiple days in a row, only the latest day needs to be re-run to refresh all the data.
    * If **ðŸ”‚ no**, must re-run this DAG for each day that failed to ensure that all data is captured
    * If no (without emoji), this is not all of history, but should not be re-run repeatedly (for example, it scrapes data; if you run it repeatedly, it will just scrape the same data over and over)
    * If yes, can re-run only the most recent date
* **Depends on** - The information in the table below is a high-level summary indicating general DAG execution order and explicit inter-DAG dependencies; there may be additional dependencies for individual tasks within a DAG, and there may be cases where a DAG task depends on **data** generated by another DAG task but the task-task dependency is not formally specified.
    * If N/A, this DAG is the bottom of the dependency ladder and can run independently.
    * If populated, this DAG has upstream dependencies that must complete before it will run


DAGs are listed in alphabetical order, as they appear in the Airflow UI.

| DAG | Safe after 24h | `depends_ on_past` | All of history | Depends on | Notes |
| --- | --- | --- | --- | --- | --- |
`airtable_loader` | **â›” No*** | No | **ðŸ”‚ No** | N/A | All tasks are unsafe after 24 hours |
`airtable_loader_v2` | Yes | No | No* | N/A | Don't need to rerun more than once if multiple failures; scrapes data that is correctly timestamped |
`airtable_views` | Yes | No | Yes* | `airtable_ loader` | Latest-only data |
`amplitude_benefits` | Yes | No | **ðŸ”‚ No** | N/A | |
`check_feed_aggregators` | **â›” No** | No | **ðŸ”‚ No** | N/A | |
`create_external_tables` | N/A | N/A | N/A | N/A | Once-only (defines external tables); does not generally need to be re-run  |
`deploy_dbt_docs` | Yes | No | N/A | N/A | Manual job to deploy dbt docs to Metabase and Netlify |
`download_gtfs_schedule_v2` | Yes | No | No* | N/A | Don't need to rerun more than once if multiple failures; scrapes data that is correctly timestamped  |
`gtfs_downloader. generate_provider_list` and `gtfs_downloader. download_data` | **â›” No*** | No | **ðŸ”‚ No** | N/A |  |
`gtfs_downloader. email_failures` and `gtfs_downloader. validate_gcs_bucket` | Yes | No | **ðŸ”‚ No** | N/A |  |
`gtfs_loader` | Yes | No | **ðŸ”‚ No** | `gtfs_ downloader`* | Technically also depends on `gtfs_schedule_history`, not usually an issue |
`gtfs_schedule_history` | N/A | N/A | N/A | N/A | Once-only (defines external tables); does not generally need to be re-run |
`gtfs_schedule_history2` | Yes | **ðŸ“† Yes** | **ðŸ”‚ No** | `gtfs_ loader` | |
`parse_and_validate_rt` | Yes | No | **ðŸ”‚ No** | N/A | |
`parse_and_validate_rt_v2` | Yes | No | **ðŸ”‚ No** | N/A | |
`payments_loader` | Yes | No | Yes | N/A | |
`rt_loader` | Yes | No | **ðŸ”‚ No** | `gtfs_ loader` | |
`rt_loader_files` | Yes | No | **ðŸ”‚ No** | N/A | |
`sandbox` | N/A | N/A | N/A | N/A | Testing only; does not need to be re-run |
`transform_warehouse` | Yes | No | Yes | N/A | Runs dbt warehouse |
`unzip_and_validate_gtfs_schedule_` | Yes | No | **ðŸ”‚ No** | N/A | |

### Deprecated DAGs

The following DAGs are still listed in the Airflow UI even though they are **deprecated or indefinitely paused**. They never need to be re-run.

* `check_data_freshness`
* `gtfs_schedule`
* `gtfs_views_staging`
* `gtfs_views`
* `parse_rt`
* `rt_timestamp_fix`
* `rt_views`
* `transitstacks_loader`
* `transitstacks_views`

## Dependency diagram

In addition to the tabular view above, here is a diagram representing DAG dependencies.

```{mermaid}
  graph TD;
      airtable_loader_v2;
      airtable_loader-->airtable_views;
      amplitude_benefits;
      download_gtfs_schedule_v2;
      gtfs_downloader-->gtfs_loader;
      gtfs_schedule_history-->gtfs_loader;
      gtfs_loader-->gtfs_schedule_history2;
      payments_loader;
      gtfs_loader-->rt_loader;
      sandbox;
      create_external_tables;
      check_feed_aggregators;
      parse_and_validate_rt;
      transform_warehouse;
```

## Task-level considerations

Some tasks have unique considerations, beyond the requirements of their overall DAG.

* **`PodOperators`**: When restarting a failed `PodOperator` run, check the logs before restarting. If the logs show any indication that the prior run's pod was not killed (for example, if the logs cut off abruptly without showing an explicit task failure), you should check that the pod associated with the failed run task has in fact been killed before clearing or restarting the Airflow task. If you don't know how to check a pod status, please ask in the `#data-infra` channel on Slack before proceeding.

## Backfilling from the command line

From time-to-time some DAGs may need to be re-ran in order to populate new data.

More will be added later, but whenever backfilling of DAG tasks need to happen, the following command can be ran, subject to the considerations outlined above:

```shell
gcloud composer environments run calitp-airflow-prod --location=us-west2 backfill -- --start_date 2021-04-18 --end_date 2021-11-03 -x --reset_dagruns -y -t "gtfs_schedule_history_load" -i gtfs_loader
```
