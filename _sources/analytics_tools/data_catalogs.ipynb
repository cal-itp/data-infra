{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e991bde4",
   "metadata": {},
   "source": [
    "(data-catalogs)=\n",
    "\n",
    "# Using Data Catalogs\n",
    "\n",
    "One major difficulty with conducting reproducible analyses is the location of data. If a data analyst downloads a CSV on their local system, but does not document its provenance or access, the analysis becomes very difficult to reproduce.\n",
    "\n",
    "One strategy to deal with this is to create data catalogs for projects, which describe the data sources used and how to access them. Our team uses open data sources, database, and any other dataset that needs to be versioned must be stored in Google Cloud Storage (GCS) buckets. A data catalog that documents these heterogeneous sources simplifies and streamlines the \"read\" side of reading and writing data.\n",
    "\n",
    "Each task sub-folder within the `data-analyses` repo should come with its own data catalog, documenting the data sources used within the notebooks and scripts.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Data Catalogs with [Intake](#intake)\n",
    "2. [Open Data Portals](#open-data-portals)\n",
    "3. [Google Cloud Storage](#google-cloud-storage) (GCS) Buckets\n",
    "4. [Sample Data Catalog](#sample-data-catalog)\n",
    "\n",
    "### Intake\n",
    "\n",
    "Data analysts tend to load their data from many heterogeneous sources (Databases, CSVs, JSON, etc), but at the end of the day, they often end up with the data in dataframes or numpy arrays. One tool for managing that in Python is the relatively new project `intake`. Intake provides a way to make data catalogs can then be used load sources into dataframes and arrays. These catalogs are plain text and can then be versioned and published, allowing for more ergonomic documentation of the data sources used for a project.\n",
    "\n",
    "`intake-dcat` is a tool for allowing intake to more easily interact with DCAT catalogs commonly found on open data portals.\n",
    "\n",
    "Refer to this [sample-catalog.yml](sample-catalog) to see how various data sources and file types are documented. Each dataset is given a human-readable name, with optional metadata associated.\n",
    "\n",
    "File types that work within GCS buckets, URLs, or DCATs (open data catalogs):\n",
    "\n",
    "- Tabular: CSV, parquet\n",
    "- Geospatial: zipped shapefile, GeoJSON, geoparquet\n",
    "\n",
    "To open the catalog in a Jupyter Notebook:\n",
    "\n",
    "```python\n",
    "import intake\n",
    "\n",
    "catalog = intake.open_catalog(\"./sample-catalog.yml\")\n",
    "\n",
    "# To open multiple catalog YML files\n",
    "catalog = intake.open_catalog(\"./*.yml\")\n",
    "```\n",
    "\n",
    "### Open Data Portals\n",
    "\n",
    "Open data portals (such as the CA Open Data Portal and CA Geoportal) usually provide a DCAT catalog for their datasets, including links for downloading them and metadata describing them. Many civic data analysis projects end up using these open datasets. When they do, it should be clearly documented.\n",
    "\n",
    "- To input a dataset from an open data portal, find the dataset's identifier for the `catalog.yml`.\n",
    "- Ex: The URL for CA Open Data Portal is: https://data.ca.gov.\n",
    "- Navigate to the corresponding `data.json` file at https://data.ca.gov/data.json\n",
    "- Each dataset has associated metadata, including `accessURL`, `landingPage`, etc. Find the dataset's `identifier`, and input that as the catalog item.\n",
    "\n",
    "```yaml\n",
    "# Catalog item\n",
    "ca_open_data:\n",
    "driver: dcat\n",
    "args:\n",
    "  url: https://data.ca.gov/data.json\n",
    "  items:\n",
    "      cdcr_population_covid_tracking: 4a9a896a-e64e-48c2-bb35-5589f80e7c52\n",
    "```\n",
    "\n",
    "To import this dataset as a dataframe within the notebook:\n",
    "\n",
    "```python\n",
    "df = catalog.ca_open_data.cdcr_population_covid_tracking.read()\n",
    "```\n",
    "\n",
    "(catalogue-cloud-storage)=\n",
    "\n",
    "### Google Cloud Storage\n",
    "\n",
    "When putting GCS files into the catalog, note that geospatial datasets (zipped shapefiles, GeoJSONs) require the additional `use_fsspec: true` argument compared to tabular datasets (parquets, CSVs). Geoparquets, the exception, are catalogued like tabular datasets.\n",
    "\n",
    "Opening geospatial datasets through `intake` is the easiest way to import these datasets within a Jupyter Notebook. Otherwise, `geopandas` can read the geospatial datasets that are locally saved or downloaded first from the bucket, but not directly with a GCS file path. Refer to \\[storing data\\](Connecting to the Warehouse) to set up your Google authentication.\n",
    "\n",
    "```yaml\n",
    "lehd_federal_jobs_by_tract:\n",
    "    driver: parquet\n",
    "    description: LEHD Workplace Area Characteristics (WAC) federal jobs by census tract.\n",
    "    args:\n",
    "      urlpath: gs://calitp-analytics-data/data-analyses/bus_service_increase/wac_fed_tract.parquet\n",
    "      engine: pyarrow\n",
    "test_csv:\n",
    "    driver: csv\n",
    "    description: Description\n",
    "    args:\n",
    "      urlpath: https://raw.githubusercontent.com/CityOfLosAngeles/covid19-indicators/master/data/ca_county_pop_crosswalk.csv\n",
    "test_zipped_shapefile:\n",
    "    driver: shapefile\n",
    "    description: LA Metro rail lines\n",
    "    args:\n",
    "      urlpath: gs://calitp-analytics-data/test_zipped_shapefile.zip\n",
    "      use_fsspec: true\n",
    "test_geoparquet:\n",
    "    driver: geoparquet\n",
    "    description: Description\n",
    "    args:\n",
    "      urlpath: gs://calitp-analytics-data/test_geoparquet.parquet\n",
    "```\n",
    "\n",
    "To import each of these files as dataframes or geodataframes:\n",
    "\n",
    "```python\n",
    "df1 = catalog.lehd_federal_jobs_by_tract.read()\n",
    "\n",
    "df2 = catalog.test_csv.read()\n",
    "\n",
    "gdf1 = catalog.test_zipped_shapefile.read()\n",
    "\n",
    "gdf2 = catalog.test_geoparquet.read()\n",
    "```\n",
    "\n",
    "(sample-catalog)=\n",
    "\n",
    "# Sample Data Catalog\n",
    "\n",
    "```{literalinclude} sample-catalog.yml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}