{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7bfda7",
   "metadata": {},
   "source": [
    "(storing-new-data)=\n",
    "\n",
    "# Storing Data During Analysis\n",
    "\n",
    "Our team uses Google Cloud Storage (GCS) buckets, specifically the `calitp-analytics-data` bucket, to store other datasets for analyses. GCS can store anything, of arbitrary object size and shape. It’s like a giant folder in the cloud. You can use it to store CSVs, parquets, pickles, videos, etc. **Within the bucket, the `data-analyses` folder with its sub-folders corresponds to the `data-analyses` GitHub repo with its sub-folders. Versioned data for a task should live within the correct folders.**\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Storing New Data - Screencast](storing-new-data-screencast)\n",
    "3. [Uploading Data from a Notebook](uploading-from-notebook)\n",
    "   <br> - [Tabular Data](#tabular-data)\n",
    "   <br> - [Parquet](#parquet)\n",
    "   <br> - [CSV](#csv)\n",
    "   <br> - [Geospatial Data](#geospatial-data)\n",
    "   <br> - [Geoparquet](#geoparquet)\n",
    "   <br> - [Zipped shapefile](#zipped-shapefile)\n",
    "   <br> - [GeoJSON](#geojson)\n",
    "4. [Uploading data in Google Cloud Storage](in-gcs)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Currently, report data can be stored in the `calitp-analytics-data` bucket in Google Cloud Storage.\n",
    "\n",
    "In order to save data being used in a report, you can use two methods:\n",
    "\n",
    "- Using code in your notebook to upload the data.\n",
    "- Using the Google Cloud Storage web UI to manually upload.\n",
    "\n",
    "Watch the screencast below and read the additional information to begin.\n",
    "\n",
    "**Note**: To access Google Cloud Storage you will need to have set up your Google authentication. If you have yet to do so, [follow these instructions](connecting-to-warehouse).\n",
    "\n",
    "(storing-new-data-screencast)=\n",
    "\n",
    "## Storing New Data - Screencast\n",
    "\n",
    "<div style=\"position: relative; padding-bottom: 62.5%; height: 0;\"><iframe src=\"https://www.loom.com/embed/51d22876ab6d4d35a39f18e8f6d5f11d\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"></iframe></div>\n",
    "\n",
    "(uploading-from-notebook)=\n",
    "\n",
    "## Uploading Data from a Notebook\n",
    "\n",
    "In order to begin, import the following libraries in your notebook and set the `fs` variable\n",
    "\n",
    "```python\n",
    "import geopandas as gpd\n",
    "import gcsfs\n",
    "import pandas as pd\n",
    "\n",
    "from calitp_data_analysis import get_fs\n",
    "fs = get_fs()\n",
    "```\n",
    "\n",
    "### Tabular Data\n",
    "\n",
    "While GCS can store CSVs, parquets, Excel spreadsheets, etc, parquets are the preferred file type. Interacting with tabular datasets in GCS is fairly straightforward and is handled well by `pandas`.\n",
    "\n",
    "#### Parquet\n",
    "\n",
    "Parquet is an “open source columnar storage format for use in data analysis systems.” Columnar storage is more efficient as it is easily compressed and the data is more homogenous. CSV files utilize a row-based storage format which is harder to compress, a reason why Parquets files are preferable for larger datasets. Parquet files are faster to read than CSVs, as they have a higher querying speed and preserve datatypes (ie, Number, Timestamps, Points). They are best for intermediate data storage and large datasets (1GB+) on most any on-disk storage. This file format is also good for passing dataframes between Python and R. A similar option is feather.\n",
    "\n",
    "One of the downsides to Parquet files is the inability to quickly look at the dataset in GUI based (Excel, QGIS, etc.) programs. Parquet files also lack built-in support for categorical data.\n",
    "\n",
    "```python\n",
    "df = pd.read_parquet('gs://calitp-analytics-data/data-analyses/task-subfolder/test.parquet')\n",
    "\n",
    "df.to_parquet('gs://calitp-analytics-data/data-analyses/task-subfolder/test.parquet')\n",
    "```\n",
    "\n",
    "#### CSV\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('gs://calitp-analytics-data/data-analyses/task-subfolder/test.csv')\n",
    "\n",
    "df.to_csv('gs://calitp-analytics-data/data-analyses/task-subfolder/test.parquet')\n",
    "```\n",
    "\n",
    "### Geospatial Data\n",
    "\n",
    "Geospatial data may require a little extra work, due to how `geopandas` and GCS interacts.\n",
    "\n",
    "#### Geoparquet\n",
    "\n",
    "Importing geoparquets directly from GCS works with `geopandas`, but exporting geoparquets into GCS requires an extra step of uploading.\n",
    "\n",
    "```python\n",
    "\n",
    "GCS_FOLDER = \"gs://calitp-analytics-data/data-analyses/task-subfolder/\"\n",
    "\n",
    "gdf = gpd.read_parquet(f\"{GCS_FOLDER}.parquet\")\n",
    "\n",
    "# Save the geodataframe to your local filesystem\n",
    "gdf.to_parquet(\"./my-geoparqet.parquet\")\n",
    "\n",
    "# Put the local file into the GCS bucket\n",
    "fs.put(\"./my-geoparquet.parquet\", f\"{GCS_FOLDER}my-geoparquet.parquet\")\n",
    "```\n",
    "\n",
    "Or, use the `calitp_data_analysis` package that lives in [data-infra](https://github.com/cal-itp/data-infra/tree/main/packages/calitp-data-analysis/calitp_data_analysis)\n",
    "\n",
    "```python\n",
    "from calitp_data_analysis import utils\n",
    "\n",
    "utils.geoparquet_gcs_export(\n",
    "    gdf,\n",
    "    GCS_FOLDER,\n",
    "    \"my-geoparquet\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Zipped Shapefile\n",
    "\n",
    "Refer to the [data catalogs doc](catalogue-cloud-storage) to list a zipped shapefile, and read in the zipped shapefile with the `intake` method. Zipped shapefiles saved in GCS cannot be read in directly using `geopandas`.\n",
    "\n",
    "#### GeoJSON\n",
    "\n",
    "Refer to the [data catalogs doc](catalogue-cloud-storage) to list a GeoJSON, and read in the GeoJSON with the `intake` method. GeoJSONs saved in GCS cannot be read in directly using `geopandas`.\n",
    "\n",
    "Use the `calitp_data_analysis` package to read in or export geojsons.\n",
    "\n",
    "```python\n",
    "from calitp_data_analysis import utils\n",
    "\n",
    "GCS_FOLDER = \"gs://calitp-analytics-data/data-analyses/task-subfolder/\"\n",
    "\n",
    "gdf = utils.read_geojson(\n",
    "    GCS_FOLDER,\n",
    "    \"my-geojson.geojson\",\n",
    "    geojson_type = \"geojson\",\n",
    "    save_locally = True\n",
    ")\n",
    "\n",
    "utils.geojson_gcs_export(\n",
    "    gdf,\n",
    "    GCS_FOLDER,\n",
    "    \"my-geojson.geojson\",\n",
    "    geojson_type = \"geojson\",\n",
    ")\n",
    "```\n",
    "\n",
    "(in-gcs)=\n",
    "\n",
    "## Uploading data in Google Cloud Storage\n",
    "\n",
    "You can access the cloud bucket from the web from https://console.cloud.google.com/storage/browser/calitp-analytics-data.\n",
    "\n",
    "See the above screencast for a walkthrough of using the bucket."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "source_map": [
   14
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}