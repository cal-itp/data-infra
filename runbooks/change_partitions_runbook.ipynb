{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21efbc-f3bd-4278-b356-994b2ddc77e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do this first and restart kernel\n",
    "# need version of the GTFSDownloadConfig class that has optional config.extracted_at\n",
    "%pip install calitp==\"2022.9.13a0\"\n",
    "%pip install pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ac1b1-4492-4db0-a318-754e3d906bcf",
   "metadata": {},
   "source": [
    "# Classes & config\n",
    "classes from https://github.com/cal-itp/data-infra/blob/airtable-extracted-ts/airflow/dags/download_gtfs_schedule_v2/download_schedule_feeds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141de325-e20a-47d5-ac21-7c4470276224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "import os\n",
    "os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"] = \"test-calitp-gtfs-schedule-raw-v2\"\n",
    "OLD_SCHEDULE_RAW_BUCKET = \"test-calitp-gtfs-schedule-raw\"\n",
    "SCHEDULE_RAW_BUCKET = \"test-calitp-gtfs-schedule-raw-v2\"\n",
    "JSONL_EXTENSION = \".jsonl\"\n",
    "\n",
    "from calitp.storage import get_fs\n",
    "fs = get_fs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d687ba-7e51-48d8-83c7-84f89ee0b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract, ProcessingOutcome, PartitionedGCSArtifact\n",
    "from typing import Optional, ClassVar, List\n",
    "from pydantic import validator\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = OLD_SCHEDULE_RAW_BUCKET\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome]\n",
    "\n",
    "    @validator(\"filename\", allow_reuse=True)\n",
    "    def is_jsonl(cls, v):\n",
    "        assert v.endswith(JSONL_EXTENSION)\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()\n",
    "\n",
    "    @property\n",
    "    def successes(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if outcome.success]\n",
    "\n",
    "    @property\n",
    "    def failures(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if not outcome.success]\n",
    "\n",
    "    # TODO: I dislike having to exclude the records here\n",
    "    #   I need to figure out the best way to have a single type represent the \"metadata\" of\n",
    "    #   the content as well as the content itself\n",
    "    def save(self, fs):\n",
    "        self.save_content(\n",
    "            fs=fs,\n",
    "            content=\"\\n\".join(o.json() for o in self.outcomes).encode(),\n",
    "            exclude={\"outcomes\"},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e571987-2749-4ddf-910b-a548a0abcaee",
   "metadata": {},
   "source": [
    "# Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6763d9c-bbac-47d3-9ec5-ee119905cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "from tqdm.notebook import tqdm\n",
    "results_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/download_schedule_feed_results/', recursive=True)\n",
    "results_files = [file for file in results_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "\n",
    "data_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/', recursive=True)\n",
    "data_files = [file for file in data_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "\n",
    "results_paths = [(path, *path.split(\"/\")) for path in results_files]\n",
    "data_paths = [(path, *path.split(\"/\")) for path in data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea8da-d5ee-4d07-ae4b-22e24a045e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: use the actual calitp type -- requires pip installing newest calitp version \n",
    "# construct an extract with the new config type from the existing metadata\n",
    "\n",
    "# make a dict mapping ts+airtable record to b64url -- use this to link extract with outcome to populate config.url\n",
    "\n",
    "# schedule outcomes files \n",
    "\n",
    "import base64\n",
    "import pendulum\n",
    "import json\n",
    "from datetime import datetime\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract\n",
    "\n",
    "# old path, new path, new extract (i.e., new metadata)\n",
    "moves = []\n",
    "\n",
    "# map ts & airtable record to base64 url \n",
    "airtable_record_mapper = {}\n",
    "\n",
    "# new results objects\n",
    "results_to_save = []\n",
    "\n",
    "for og_path, bucket, table, dt, base64url, ts, filename in data_paths:\n",
    "    old_extract = json.loads(fs.getxattr(path=f\"gs://{og_path}\", attr=\"PARTITIONED_ARTIFACT_METADATA\"))\n",
    "    \n",
    "    old_config = old_extract.pop(\"config\")\n",
    "    \n",
    "    new_config = GTFSDownloadConfig(\n",
    "        name = old_config.get(\"name\"),\n",
    "        auth_query_params = {old_config.get(\"authorization_url_parameter_name\"): old_config.get(\"url_secret_key_name\")} if old_config.get(\"authorization_url_parameter_name\") else {},\n",
    "        auth_headers = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")} if old_config.get(\"authorization_header_parameter_name\") else {},\n",
    "        feed_type = old_config.get(\"data\"),\n",
    "        url = base64.urlsafe_b64decode(base64url.replace(\"base64_url=\",\"\")).decode(),\n",
    "        schedule_url_for_validation = None\n",
    "        )\n",
    "    \n",
    "    new_extract = GTFSScheduleFeedExtract(\n",
    "        config = new_config,\n",
    "        **old_extract\n",
    "        )\n",
    "    \n",
    "    moves.append((og_path, new_entity.path, new_config, new_extract))\n",
    "    airtable_record_mapper[old_config[\"id\"]] = (new_config, new_extract)\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f52bc-465f-4088-b9d9-5a29607852f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for og_path, new_path in tqdm(moves):\n",
    "    #print(og_path, new_path)\n",
    "    #break\n",
    "    fs.mv(og_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03756990-89b3-4679-9880-a9d7a30a8e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
