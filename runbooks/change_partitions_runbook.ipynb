{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21efbc-f3bd-4278-b356-994b2ddc77e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do this first and restart kernel\n",
    "# need version of the GTFSDownloadConfig class that has optional config.extracted_at\n",
    "%pip install calitp==\"2022.9.13a0\"\n",
    "%pip install pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb666f-6645-482f-8cda-018df4c06d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calitp.storage import get_fs\n",
    "fs = get_fs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ac1b1-4492-4db0-a318-754e3d906bcf",
   "metadata": {},
   "source": [
    "# Classes & config\n",
    "classes from https://github.com/cal-itp/data-infra/blob/airtable-extracted-ts/airflow/dags/download_gtfs_schedule_v2/download_schedule_feeds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141de325-e20a-47d5-ac21-7c4470276224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "OLD_SCHEDULE_RAW_BUCKET = \"test-calitp-gtfs-schedule-raw\"\n",
    "SCHEDULE_RAW_BUCKET = \"test-calitp-gtfs-schedule-raw-v2\"\n",
    "JSONL_EXTENSION = \".jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d687ba-7e51-48d8-83c7-84f89ee0b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract, ProcessingOutcome, PartitionedGCSArtifact\n",
    "from typing import Optional, ClassVar, List\n",
    "from pydantic import validator\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = OLD_SCHEDULE_RAW_BUCKET\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome]\n",
    "\n",
    "    @validator(\"filename\", allow_reuse=True)\n",
    "    def is_jsonl(cls, v):\n",
    "        assert v.endswith(JSONL_EXTENSION)\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()\n",
    "\n",
    "    @property\n",
    "    def successes(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if outcome.success]\n",
    "\n",
    "    @property\n",
    "    def failures(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if not outcome.success]\n",
    "\n",
    "    # TODO: I dislike having to exclude the records here\n",
    "    #   I need to figure out the best way to have a single type represent the \"metadata\" of\n",
    "    #   the content as well as the content itself\n",
    "    def save(self, fs):\n",
    "        self.save_content(\n",
    "            fs=fs,\n",
    "            content=\"\\n\".join(o.json() for o in self.outcomes).encode(),\n",
    "            exclude={\"outcomes\"},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e571987-2749-4ddf-910b-a548a0abcaee",
   "metadata": {},
   "source": [
    "# Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6763d9c-bbac-47d3-9ec5-ee119905cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "from tqdm.notebook import tqdm\n",
    "results_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/download_schedule_feed_results/', recursive=True)\n",
    "results_files = [file for file in results_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "len(results_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8b30a-2577-4366-8343-c3503aa55329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea8da-d5ee-4d07-ae4b-22e24a045e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: use the actual calitp type -- requires pip installing newest calitp version \n",
    "# construct an extract with the new config type from the existing metadata\n",
    "\n",
    "# make a dict mapping ts+airtable record to b64url -- use this to link extract with outcome to populate config.url\n",
    "\n",
    "# schedule outcomes files \n",
    "\n",
    "import base64\n",
    "import pendulum\n",
    "import json\n",
    "from datetime import datetime\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract\n",
    "\n",
    "# old path, new path, new metadata\n",
    "moves = []\n",
    "\n",
    "# just a list of results objects\n",
    "results_to_save = []\n",
    "\n",
    "for results_file in tqdm(results_files):\n",
    "    with fs.open(results_file) as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # load outcomes rows just as json, converting exception strings to exceptions since this is required in both versions \n",
    "    outcomes_json = []\n",
    "    for row in content.decode().splitlines():\n",
    "        content_json = {**json.loads(row)}\n",
    "        if content_json[\"exception\"]:\n",
    "            content_json[\"exception\"] = Exception(content_json[\"exception\"])\n",
    "        outcomes_json.append(content_json)\n",
    "    \n",
    "    outcomes = []\n",
    "    \n",
    "    # attempt to load with current version of outcomes\n",
    "    # this will only work for a few days in September that use the new format\n",
    "    # will not work in prod \n",
    "    try:\n",
    "        for outcome_json in outcomes_json:\n",
    "            new_outcome = GTFSDownloadOutcome(**outcome_json)\n",
    "            outcomes.append(new_outcome)\n",
    "        input_results = DownloadFeedsResult(outcomes=outcomes, **json.loads(fs.getxattr(path=f\"gs://{results_file}\", attr=\"PARTITIONED_ARTIFACT_METADATA\")))\n",
    "        results_to_save.append(input_results)\n",
    "        \n",
    "        for outcome in tqdm(outcomes):\n",
    "            if outcome.success:\n",
    "                ts_string = outcome.extract.ts.to_iso8601_string()\n",
    "                dt_string = outcome.extract.ts.to_date_string()\n",
    "                base64_url = base64.urlsafe_b64encode(outcome.config.url.encode()).decode()\n",
    "\n",
    "                extract = outcome.extract\n",
    "\n",
    "                old_partitions_old_bucket_path = f\"gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/dt={dt_string}/base64_url={base64_url}/ts={ts_string}/{outcome.extract.filename}\"\n",
    "                new_partitions_old_bucket_path = f\"gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/dt={dt_string}/ts={ts_string}/base64_url={base64_url}/{outcome.extract.filename}\"\n",
    "                new_path = f\"gs://{SCHEDULE_RAW_BUCKET}/schedule/dt={dt_string}/ts={ts_string}/base64_url={base64_url}/{outcome.extract.filename}\"\n",
    "                if fs.exists(old_partitions_old_bucket_path):\n",
    "                    moves.append((old_partitions_old_bucket_path, new_path, extract))\n",
    "                if fs.exists(new_partitions_old_bucket_path):\n",
    "                    moves.append((new_partitions_old_bucket_path, new_path, extract))\n",
    "    except Exception as e:\n",
    "        # this is the loop we will use in prod, where nothing has the new config stuff \n",
    "        try: \n",
    "            for outcome_json in outcomes_json:\n",
    "                # handle config specific stuff\n",
    "                old_config = outcomes_json.get(\"config\")\n",
    "                new_config = {}\n",
    "                new_config[\"extracted_at\"] = \"\"\n",
    "                new_config[\"name\"] = old_config.get(\"name\")\n",
    "\n",
    "                if old_config.get(\"authorization_url_parameter_name\"):\n",
    "                    new_config[\"auth_query_params\"] = {old_config.get(\"authorization_url_parameter_name\"): old_config.get(\"url_secret_key_name\")}\n",
    "                else:\n",
    "                    new_config[\"auth_query_params\"] = {}\n",
    "                if old_config.get(\"authorization_header_parameter_name\"):\n",
    "                    new_config[\"auth_headers\"] = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")}\n",
    "                else: \n",
    "                    new_config[\"auth_headers\"] = {}\n",
    "                # what to do with these\n",
    "                # or should new_config[\"url\"] = reverse engineered from b64 url? \n",
    "                # yes use b64\n",
    "                new_config[\"feed_type\"] = old_config.get(\"data\")\n",
    "                new_config[\"url\"] = old_config.get(\"uri\")\n",
    "                new_config[\"schedule_url_for_validation\"] = None\n",
    "\n",
    "                #overall metadata\n",
    "                new_metadata = {}\n",
    "                for shared_key in [\"filename\", \"ts\", \"response_code\", \"response_headers\"]:\n",
    "                    new_metadata[shared_key] = old_metadata.get(shared_key)\n",
    "            \n",
    "        new_metadata[\"config\"] = new_config\n",
    "                new_outcome = GTFSDownlodOutcome(\n",
    "                )\n",
    "        \n",
    "        print(results_file)\n",
    "        print(outcome)\n",
    "        print(e)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda632c6-6edb-4ccb-8d9f-a38be273eff3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62574a7-368b-4f95-a5c2-a0fbc8c8bb07",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_results.outcomes[0].extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1974ff-c248-4638-9d1e-8a1e33a67f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for og_path, bucket, table, dt, base64url, ts, filename in paths:\n",
    "    pdt = pendulum.parse(dt.replace(\"dt=\", \"\"), exact=True)\n",
    "    assert isinstance(pdt, pendulum.Date)\n",
    "    if \"ts\" in ts:\n",
    "        pts = pendulum.parse(ts.replace(\"ts=\", \"\"), exact=True)\n",
    "        assert isinstance(pts, pendulum.DateTime)\n",
    "    elif \"time\" in ts:\n",
    "        ptime = pendulum.parse(ts.replace(\"time=\", \"\"), exact=True)\n",
    "        pts = pendulum.instance(datetime.combine(pdt, ptime))\n",
    "        ts = f\"ts={pts.to_iso8601_string()}\"\n",
    "        assert isinstance(pts, pendulum.DateTime)\n",
    "    new_path = \"/\".join([bucket, table, dt, ts, base64url, filename])\n",
    "    try:\n",
    "        old_metadata = json.loads(fs.getxattr(og_path, \"PARTITIONED_ARTIFACT_METADATA\"))\n",
    "        \n",
    "        # handle config specific stuff\n",
    "        old_config = old_metadata.get(\"config\")\n",
    "        new_config = {}\n",
    "        # TODO: what to put here? leave null after calitp update \n",
    "        new_config[\"extracted_at\"] = \"\"\n",
    "        new_config[\"name\"] = old_config.get(\"name\")\n",
    "        \n",
    "        # these are not getting anything.... how was auth info stored before?\n",
    "        if old_config.get(\"authorization_url_parameter_name\"):\n",
    "            new_config[\"auth_query_params\"] = {old_config.get(\"authorization_URL_parameter_name\"): old_config.get(\"URL_secret_key_name\")}\n",
    "        else:\n",
    "            new_config[\"auth_query_params\"] = {}\n",
    "        if old_config.get(\"authorization_header_parameter_name\"):\n",
    "            new_config[\"auth_headers\"] = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")}\n",
    "        else: \n",
    "            new_config[\"auth_headers\"] = {}\n",
    "        # what to do with these\n",
    "        # or should new_config[\"url\"] = reverse engineered from b64 url? \n",
    "        # yes use b64\n",
    "        new_config[\"feed_type\"] = old_config.get(\"data\")\n",
    "        new_config[\"uri\"] = old_config.get(\"uri\")\n",
    "        new_config[\"pipeline_url\"] = old_config.get(\"pipeline_url\")\n",
    "        new_config[\"schedule_url_for_validation\"] = None\n",
    "        \n",
    "        #overall metadata\n",
    "        new_metadata = {}\n",
    "        for shared_key in [\"filename\", \"ts\", \"response_code\", \"response_headers\"]:\n",
    "            new_metadata[shared_key] = old_metadata.get(shared_key)\n",
    "            \n",
    "        new_metadata[\"config\"] = new_config\n",
    "        \n",
    "    except KeyError as e:\n",
    "        new_metadata = None\n",
    "    moves.append((og_path, new_path, new_metadata))\n",
    "moves[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f52bc-465f-4088-b9d9-5a29607852f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for og_path, new_path in tqdm(moves):\n",
    "    #print(og_path, new_path)\n",
    "    #break\n",
    "    fs.mv(og_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03756990-89b3-4679-9880-a9d7a30a8e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
