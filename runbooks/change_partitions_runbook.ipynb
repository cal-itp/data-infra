{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21efbc-f3bd-4278-b356-994b2ddc77e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do this first and restart kernel\n",
    "# need version of the GTFSDownloadConfig class that has optional config.extracted_at\n",
    "%pip install calitp==\"2022.9.13a0\"\n",
    "%pip install pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ac1b1-4492-4db0-a318-754e3d906bcf",
   "metadata": {},
   "source": [
    "# Classes & config\n",
    "classes from https://github.com/cal-itp/data-infra/blob/airtable-extracted-ts/airflow/dags/download_gtfs_schedule_v2/download_schedule_feeds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141de325-e20a-47d5-ac21-7c4470276224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration / setup\n",
    "import os\n",
    "os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"] = \"test-calitp-gtfs-schedule-raw-v2\"\n",
    "OLD_SCHEDULE_RAW_BUCKET = \"test-calitp-gtfs-schedule-raw\"\n",
    "JSONL_EXTENSION = \".jsonl\"\n",
    "\n",
    "from calitp.storage import get_fs\n",
    "fs = get_fs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d687ba-7e51-48d8-83c7-84f89ee0b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract, ProcessingOutcome, PartitionedGCSArtifact\n",
    "from typing import Optional, ClassVar, List\n",
    "from pydantic import validator\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = OLD_SCHEDULE_RAW_BUCKET\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome]\n",
    "\n",
    "    @validator(\"ts\")\n",
    "    def coerce_ts(cls, v):\n",
    "        return pendulum.instance(v)\n",
    "    @validator(\"end\")\n",
    "    def coerce_end(cls, v):\n",
    "        return pendulum.instance(v)\n",
    "    \n",
    "    @validator(\"filename\", allow_reuse=True)\n",
    "    def is_jsonl(cls, v):\n",
    "        assert v.endswith(JSONL_EXTENSION)\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()\n",
    "\n",
    "    @property\n",
    "    def successes(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if outcome.success]\n",
    "\n",
    "    @property\n",
    "    def failures(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if not outcome.success]\n",
    "\n",
    "    # TODO: I dislike having to exclude the records here\n",
    "    #   I need to figure out the best way to have a single type represent the \"metadata\" of\n",
    "    #   the content as well as the content itself\n",
    "    def save(self, fs):\n",
    "        self.save_content(\n",
    "            fs=fs,\n",
    "            content=\"\\n\".join(o.json() for o in self.outcomes).encode(),\n",
    "            exclude={\"outcomes\"},\n",
    "        )\n",
    "d = json.loads(fs.getxattr(path=\"gs://test-calitp-gtfs-schedule-raw/download_schedule_feed_results/dt=2022-09-01/ts=2022-09-01T00:00:26.548709+00:00/results.jsonl\", attr=\"PARTITIONED_ARTIFACT_METADATA\"))\n",
    "DownloadFeedsResult(outcomes=[], **d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e571987-2749-4ddf-910b-a548a0abcaee",
   "metadata": {},
   "source": [
    "# Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6763d9c-bbac-47d3-9ec5-ee119905cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "from tqdm.notebook import tqdm\n",
    "results_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/download_schedule_feed_results/', recursive=True)\n",
    "results_files = [file for file in results_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "\n",
    "data_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/', recursive=True)\n",
    "data_files = [file for file in data_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "\n",
    "results_paths = [(path, *path.split(\"/\")) for path in results_files]\n",
    "results_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea8da-d5ee-4d07-ae4b-22e24a045e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import pendulum\n",
    "import json\n",
    "from datetime import datetime\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract\n",
    "\n",
    "# record raw files to be moved\n",
    "moves = []\n",
    "\n",
    "# record results objects to be saved\n",
    "results_to_save = []\n",
    "\n",
    "# invalid records\n",
    "drops = {}\n",
    "\n",
    "pbar = tqdm(results_paths)\n",
    "for og_path, bucket, table, dt, ts, filename in pbar:\n",
    "    pbar.set_description(f\"processing {dt}\")\n",
    "    \n",
    "    # checks \n",
    "    assert table == \"download_schedule_feed_results\"\n",
    "    pdt = pendulum.parse(dt.replace(\"dt=\", \"\"), exact=True)\n",
    "    assert isinstance(pdt, pendulum.Date)\n",
    "    pts = pendulum.parse(ts.replace(\"ts=\", \"\"), exact=True)\n",
    "    assert isinstance(pts, pendulum.DateTime)\n",
    "    \n",
    "    with fs.open(og_path) as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    new_outcomes = []\n",
    "    \n",
    "    new_drops = []\n",
    "    \n",
    "    content_pbar = tqdm(content.decode().splitlines(), leave=False)\n",
    "    # load outcomes rows just as json, converting exception strings to exceptions\n",
    "    for row in content_pbar:\n",
    "        content_json = json.loads(row)\n",
    "        content_json[\"exception\"] = Exception(content_json[\"exception\"]) if content_json[\"exception\"] else None\n",
    "        \n",
    "        # if success, we have a file \n",
    "        if content_json[\"success\"]:\n",
    "            \n",
    "            extract_pts = pendulum.parse(content_json[\"extract\"][\"ts\"], exact=True)\n",
    "            assert isinstance(extract_pts, pendulum.DateTime)\n",
    "            \n",
    "            assert content_json[\"extract\"][\"config\"][\"uri\"] == content_json[\"airtable_record\"][\"uri\"], f'extract uri {content_json[\"extract\"][\"config\"][\"uri\"]} differs from airtable record uri: {content_json[\"airtable_record\"][\"uri\"]}'\n",
    "\n",
    "            old_config = content_json[\"extract\"].pop(\"config\")\n",
    "            old_extract = content_json[\"extract\"]\n",
    "            base64url = base64.urlsafe_b64encode(old_config[\"uri\"].encode()).decode() \n",
    "            old_extract_path = f'gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/dt={extract_pts.to_date_string()}/base64_url={base64url}/ts={extract_pts.to_iso8601_string()}/{old_extract[\"filename\"]}'\n",
    "            \n",
    "            assert fs.exists(old_extract_path), f\"error: {old_extract_path} does not exist; mismatch between outcomes and actual files\"\n",
    "\n",
    "            new_config = GTFSDownloadConfig(\n",
    "                    name = old_config.get(\"name\"),\n",
    "                    auth_query_params = {old_config.get(\"authorization_url_parameter_name\"): old_config.get(\"url_secret_key_name\")} if old_config.get(\"authorization_url_parameter_name\") else {},\n",
    "                    auth_headers = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")} if old_config.get(\"authorization_header_parameter_name\") else {},\n",
    "                    feed_type = old_config.get(\"data\"),\n",
    "                    url = old_config[\"uri\"],\n",
    "                    schedule_url_for_validation = None\n",
    "                    )\n",
    "\n",
    "            new_extract = GTFSScheduleFeedExtract(\n",
    "                config = new_config,\n",
    "                **old_extract\n",
    "                )\n",
    "\n",
    "            new_outcome = GTFSDownloadOutcome(\n",
    "                success = content_json[\"success\"],\n",
    "                exception = content_json[\"exception\"],\n",
    "                config = new_config,\n",
    "                extract = new_extract\n",
    "            )\n",
    "            \n",
    "            new_outcomes.append(new_outcome)\n",
    "            moves.append((old_extract_path, f'gs://{new_extract.path}', new_extract))\n",
    "            \n",
    "        else:\n",
    "            old_config = content_json[\"airtable_record\"]\n",
    "            \n",
    "            try:\n",
    "                new_config = GTFSDownloadConfig(\n",
    "                    name = old_config.get(\"name\"),\n",
    "                    auth_query_params = {old_config.get(\"authorization_url_parameter_name\"): old_config.get(\"url_secret_key_name\")} if old_config.get(\"authorization_url_parameter_name\") else {},\n",
    "                    auth_headers = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")} if old_config.get(\"authorization_header_parameter_name\") else {},\n",
    "                    feed_type = old_config.get(\"data\"),\n",
    "                    url = old_config[\"uri\"],\n",
    "                    schedule_url_for_validation = None\n",
    "                    )\n",
    "                \n",
    "                new_outcome = GTFSDownloadOutcome(\n",
    "                    success = content_json[\"success\"],\n",
    "                    exception = content_json[\"exception\"],\n",
    "                    config = new_config\n",
    "                    )\n",
    "                \n",
    "                new_outcomes.append(new_outcome)\n",
    "                \n",
    "            except Exception as e:\n",
    "                new_drops.append(content_json) \n",
    "            \n",
    "    \n",
    "    len_outcomes = len(new_outcomes)\n",
    "    len_drops = len(new_drops)\n",
    "    len_content = len(content.decode().splitlines())\n",
    "    assert len_outcomes + len_drops == len_content, f\"got {len_outcomes} outcomes and {len_drops} drops from {len_content} input records\"\n",
    "    new_results = DownloadFeedsResult(outcomes=new_outcomes, **json.loads(fs.getxattr(path=f\"gs://{og_path}\", attr=\"PARTITIONED_ARTIFACT_METADATA\")))\n",
    "    if len_drops:\n",
    "        drops[pdt] = new_drops\n",
    "    results_to_save.append(new_results)\n",
    "\n",
    "len_moves = len(moves)\n",
    "len_data_files = len(data_files)\n",
    "assert len_data_files == len_data_files, f\"got {len_moves} from {len_data_files}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3804f6-fdbb-421e-b76f-7d1201d7b0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moves[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a415db2-6682-44d7-a36f-e2bf26821b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_to_save[0].ts, len(results_to_save[0].outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f52bc-465f-4088-b9d9-5a29607852f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for og_path, new_path, new_extract in tqdm(moves):\n",
    "    fs.cp(og_path, new_path)\n",
    "    fs.setxattrs(path=new_path, PARTITIONED_ARTIFACT_METADATA=new_extract.json())\n",
    "for result in tqdm(results_to_save):\n",
    "    result.save(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03756990-89b3-4679-9880-a9d7a30a8e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
