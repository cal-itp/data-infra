{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00d77be-a1ff-4b7e-871f-439809803451",
   "metadata": {},
   "source": [
    "# Real historical backfill aka agencies.yml v1 to post-GTFSDownloadConfig v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e2980-2cee-49d1-8b59-2af67811da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"] = \"gs://test-calitp-gtfs-schedule-raw-v2\"\n",
    "from calitp.storage import get_fs\n",
    "from google.cloud import storage\n",
    "\n",
    "fs = get_fs()\n",
    "PARTITIONED_ARTIFACT_METADATA_KEY = \"PARTITIONED_ARTIFACT_METADATA\"\n",
    "client = storage.Client(project=\"cal-itp-data-infra\")\n",
    "fs, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeeb7bc-6a5b-496a-8a42-32de7e3b4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = pendulum.parse(\"2021-04-15\", exact=True)\n",
    "first_date_v2 = pendulum.parse(\"2022-07-07\", exact=True)\n",
    "folders = []\n",
    "for d in fs.ls(\"gs://gtfs-data-test/schedule/\"):\n",
    "    if \"T00\" not in d:\n",
    "        # skip some old ones that don't have midnight execution times, we probably shouldn't trust them?\n",
    "        continue\n",
    "    ts = pendulum.parse(d.split(\"/\")[-1])\n",
    "    if first_date <= ts.date() and ts.date() <= first_date_v2:\n",
    "        folders.append(d)\n",
    "len(folders), folders[0], folders[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc101c64-a022-4589-ac32-23a09fab5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fs.walk(\"gs://gtfs-data-test/schedule/2021-04-15T00:00:00+00:00/0_0/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e196c5-b0f4-4aae-b475-93c460c078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fs.walk(\"gs://gtfs-data-test/schedule/2021-04-15T00:00:00+00:00/232_0/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b584e-de33-487c-b155-88321664024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.stat(\"gs://gtfs-data-test/schedule/2021-04-15T00:00:00+00:00/0_0/stops.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d661db-f006-430e-8e95-0c758d09334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tuple(fs.ls(feed_dir)) for folder in tqdm(folders) for feed_dir in fs.ls(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7d728-320a-40f7-97b9-99798d171037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import ClassVar, List, Optional\n",
    "\n",
    "import pendulum\n",
    "from calitp.storage import (\n",
    "    GTFSDownloadConfig,\n",
    "    GTFSFeedType,\n",
    "    GTFSScheduleFeedExtract,\n",
    "    PartitionedGCSArtifact,\n",
    "    ProcessingOutcome,\n",
    ")\n",
    "from google.cloud import storage\n",
    "from pydantic import HttpUrl, parse_obj_as\n",
    "\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = \"gs://test-calitp-gtfs-schedule-raw-v2\"\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome]\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fbca53-1929-45cd-b147-4895666ba357",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(f\"gs://{folders[0]}/status.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader)\n",
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c62ad-8332-46cd-9fa7-c8d3dbe8dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import json\n",
    "from pydantic import ValidationError\n",
    "from zipfile import ZipFile\n",
    "from pprint import pprint\n",
    "from concurrent.futures import ThreadPoolExecutor, Future, as_completed\n",
    "\n",
    "jinja_pattern = r\"(?<=\\?)(?:\\w+)=\\w+&?\"\n",
    "\n",
    "class SkipUrl(Exception):\n",
    "    pass\n",
    "\n",
    "def zip_one_feed(feed):\n",
    "    feed_key = f\"{feed['itp_id']}_{feed['url_number']}\"\n",
    "    url = re.sub(jinja_pattern, \"\", feed[\"gtfs_schedule_url\"])\n",
    "\n",
    "    assert url and \"token\" not in url and \"api_key\" not in url\n",
    "\n",
    "    try:\n",
    "        validated_url = parse_obj_as(HttpUrl, url)\n",
    "    except ValidationError:\n",
    "        if url.startswith(\"http://.232\"):\n",
    "            raise SkipUrl\n",
    "        raise\n",
    "\n",
    "    config = GTFSDownloadConfig(\n",
    "        extracted_at=None,\n",
    "        name=feed[\"agency_name\"],  # TODO: CHANGE ME PLEASE\n",
    "        url=validated_url,\n",
    "        feed_type=GTFSFeedType.schedule,\n",
    "        schedule_url_for_validation=None,\n",
    "        auth_query_params={},\n",
    "        auth_headers={},\n",
    "    )\n",
    "\n",
    "    if feed['status'] != \"success\":\n",
    "        return GTFSDownloadOutcome(\n",
    "            success=False,\n",
    "            exception=Exception(feed['status']),\n",
    "            config=config,\n",
    "            extract=None,\n",
    "        ), None, None\n",
    "\n",
    "    feed_dir = f\"./{feed_key}\"\n",
    "    zipfile_path = f\"{feed_key}.zip\"\n",
    "    files_to_timestamps = {}\n",
    "\n",
    "    with ZipFile(zipfile_path, 'w') as zipf:\n",
    "        for current_dir, sub_dirs, files in fs.walk(f\"{folder}/{feed_key}\"):\n",
    "            if current_dir.endswith(\"processed\"):\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                file = f\"gs://{current_dir}/{file}\"\n",
    "                if file.endswith(\"validation.json\"):\n",
    "                    continue\n",
    "                files_to_timestamps[file] = pendulum.parse(fs.stat(file)['timeCreated'], exact=True).replace(microsecond=0)\n",
    "                zipf.writestr(os.path.basename(file), fs.cat(file))\n",
    "\n",
    "    with open(zipfile_path, \"rb\") as f:\n",
    "        zipfile_bytes = f.read()\n",
    "\n",
    "    first_ts = min(files_to_timestamps.values())\n",
    "    last_ts = max(files_to_timestamps.values())\n",
    "    if (last_ts - first_ts).total_seconds() > 600:\n",
    "        print(\"got weirdly long extract: \", (last_ts - first_ts))\n",
    "\n",
    "    extract = GTFSScheduleFeedExtract(\n",
    "        ts=first_ts,\n",
    "        config=config,\n",
    "        response_code=200, # this is somewhat assumed\n",
    "        filename=\"reconstructed.zip\",\n",
    "    )\n",
    "\n",
    "    outcome = GTFSDownloadOutcome(\n",
    "        success=True,\n",
    "        exception=None,\n",
    "        config=config,\n",
    "        extract=extract,\n",
    "    )\n",
    "\n",
    "    return outcome, extract, zipfile_bytes\n",
    "\n",
    "def handle_one_folder(folder, threads=32):\n",
    "    fs = get_fs()\n",
    "    outcomes_extracts_bytes = []\n",
    "\n",
    "    with fs.open(f\"gs://{folder}/status.csv\", \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    skipped = 0\n",
    "    pbar = tqdm(total=len(rows), desc=folder)\n",
    "    \n",
    "    #print(f\"Handling {folder} with {threads} threads\")\n",
    "    with ThreadPoolExecutor(max_workers=threads) as pool:\n",
    "        futures = {\n",
    "            pool.submit(\n",
    "                zip_one_feed,\n",
    "                feed=feed,\n",
    "            ): feed\n",
    "            for feed in rows\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            feed = futures[future]\n",
    "            if pbar:\n",
    "                pbar.update(1)\n",
    "            try:\n",
    "                outcomes_extracts_bytes.append(future.result())\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except SkipUrl:\n",
    "                skipped += 1\n",
    "            except Exception:\n",
    "                print(feed)\n",
    "                raise\n",
    "    \n",
    "    outcomes = [tup[0] for tup in outcomes_extracts_bytes]\n",
    "    assert len(rows) == len(outcomes) + skipped\n",
    "    return DownloadFeedsResult(\n",
    "        ts=min(outcome.extract.ts for outcome in outcomes if outcome.extract),\n",
    "        end=max(outcome.extract.ts for outcome in outcomes if outcome.extract),\n",
    "        outcomes=outcomes,\n",
    "        filename=\"results.jsonl\",\n",
    "    ), outcomes_extracts_bytes\n",
    "\n",
    "#result, outcomes_extracts_bytes = handle_one_folder(folders[0], threads=12)\n",
    "#len(result.outcomes), len(outcomes_extracts_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a093aa-8fe6-4537-b0d7-25bda9936d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "top_pbar = tqdm(total=len(folders))\n",
    "with ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    futures = {\n",
    "        pool.submit(\n",
    "            handle_one_folder,\n",
    "            folder=folder,\n",
    "        ): folder\n",
    "        for folder in folders\n",
    "    }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        folder = futures[future]\n",
    "        if pbar:\n",
    "            pbar.update(1)\n",
    "        try:\n",
    "            future.result()\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except SkipUrl:\n",
    "            skipped += 1\n",
    "        except Exception:\n",
    "            print(folder)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac8cf6-72c4-4a5b-baef-395986fa39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_one_results_blob(results_blob, dry_run=True):\n",
    "    to_copy = []\n",
    "    new_outcomes = []\n",
    "    with fs.open(f\"gs://{results_blob.bucket.name}/{results_blob.name}\") as f:\n",
    "        old_outcomes = [json.loads(line) for line in f.readlines()]\n",
    "    for result in tqdm(old_outcomes, desc=results_blob.name):\n",
    "        old_extract = result[\"extract\"]\n",
    "        old_airtable_record = result[\"airtable_record\"]\n",
    "\n",
    "        # we always need to be able to get a download config, even if there's no \"real\" file underlying it\n",
    "        new_config = GTFSDownloadConfig(\n",
    "            extracted_at=None,\n",
    "            name=old_airtable_record[\"name\"],\n",
    "            url=parse_obj_as(HttpUrl, old_airtable_record[\"pipeline_url\"]),\n",
    "            feed_type=GTFSFeedType.schedule,\n",
    "            schedule_url_for_validation=None,\n",
    "            auth_query_params={},\n",
    "            auth_headers={},\n",
    "        )\n",
    "\n",
    "        # if we had a failure, there is no extract to copy\n",
    "        if not result[\"extract\"]:\n",
    "            new_outcomes.append(\n",
    "                GTFSDownloadOutcome(\n",
    "                    success=result[\"success\"],\n",
    "                    exception=Exception(result[\"exception\"]),\n",
    "                    config=new_config,\n",
    "                    extract=None,\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # if we were successful, we should have a\n",
    "        ts = pendulum.parse(old_extract[\"ts\"])\n",
    "        dt = ts.date()\n",
    "        base64_url = base64.urlsafe_b64encode(\n",
    "            old_airtable_record[\"pipeline_url\"].encode()\n",
    "        ).decode()\n",
    "\n",
    "        # the old v2 files have url then ts, but we will be swapping them\n",
    "        old_blob_key = f\"schedule/dt={dt.to_date_string()}/base64_url={base64_url}/ts={ts.to_iso8601_string()}/{old_extract['filename']}\"\n",
    "        old_blob = results_blob.bucket.get_blob(old_blob_key)\n",
    "        if old_blob is None:\n",
    "            # print(new_config.url)\n",
    "            new_outcomes.append(\n",
    "                GTFSDownloadOutcome(\n",
    "                    success=False,\n",
    "                    exception=Exception(\"blob missing during backfill operation\"),\n",
    "                    config=new_config,\n",
    "                    extract=None,\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        assert old_blob.metadata[PARTITIONED_ARTIFACT_METADATA_KEY] == json.dumps(\n",
    "            old_extract\n",
    "        )\n",
    "\n",
    "        new_extract = GTFSScheduleFeedExtract(\n",
    "            filename=old_extract[\"filename\"],\n",
    "            config=new_config,\n",
    "            response_code=old_extract[\"response_code\"],\n",
    "            response_headers=old_extract[\"response_headers\"],\n",
    "            ts=old_extract[\"ts\"],\n",
    "        )\n",
    "\n",
    "        to_copy.append((f\"gs://{old_blob.bucket.name}/{old_blob.name}\", new_extract))\n",
    "        new_outcomes.append(\n",
    "            GTFSDownloadOutcome(\n",
    "                success=result[\"success\"],\n",
    "                exception=result[\"exception\"],\n",
    "                config=new_config,\n",
    "                extract=new_extract,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # only copy results once successful\n",
    "    old_metadata = json.loads(results_blob.metadata[PARTITIONED_ARTIFACT_METADATA_KEY])\n",
    "    new_result = DownloadFeedsResult(\n",
    "        ts=pendulum.parse(old_metadata[\"ts\"]),\n",
    "        end=pendulum.parse(old_metadata[\"end\"]),\n",
    "        outcomes=new_outcomes,\n",
    "        filename=old_metadata[\"filename\"],\n",
    "    )\n",
    "    assert len(old_outcomes) == len(new_outcomes)\n",
    "    assert len(to_copy) == len(\n",
    "        [result for result in new_result.outcomes if result.success]\n",
    "    )\n",
    "    if not dry_run:\n",
    "        pass\n",
    "    return new_result, to_copy\n",
    "\n",
    "\n",
    "results_to_save = []\n",
    "to_copies = []\n",
    "\n",
    "for results_blob in tqdm(old_v2_outcomes):\n",
    "    new_result, to_copy = handle_one_results_blob(results_blob=results_blob)\n",
    "    results_to_save.append(new_result)\n",
    "    to_copies.extend(to_copy)\n",
    "len(results_to_save), len(to_copies)\n",
    "\n",
    "for src, dst in tqdm(to_copies):\n",
    "    assert src.startswith(\"gs://test-calitp-gtfs-schedule-raw/schedule/\")\n",
    "    assert dst.path.startswith(\"gs://test-calitp-gtfs-schedule-raw-v2/schedule/\")\n",
    "    cp_args = (src, dst.path)\n",
    "    setxattr_kwargs = {\"path\": dst.path, PARTITIONED_ARTIFACT_METADATA_KEY: dst.json()}\n",
    "    # print(cp_args)\n",
    "    # print(setxattr_kwargs)\n",
    "    # fs.cp(*cp_args)\n",
    "    # fs.setxattr(**setxattr_kwargs)\n",
    "    # break\n",
    "\n",
    "for result in results_to_save:\n",
    "    assert result.path.startswith(\n",
    "        \"gs://test-calitp-gtfs-schedule-raw-v2/download_schedule_feed_results/\"\n",
    "    )\n",
    "    print(result.path)\n",
    "    # result.save(fs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
