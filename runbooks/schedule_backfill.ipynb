{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd58379-1fe5-4708-8dbf-6b8bcb7ae7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CALITP_BUCKET__GTFS_SCHEDULE_RAW'] = 'gs://test-calitp-gtfs-schedule-raw-v2'\n",
    "from calitp.storage import get_fs\n",
    "from google.cloud import storage\n",
    "\n",
    "fs = get_fs()\n",
    "PARTITIONED_ARTIFACT_METADATA_KEY = \"PARTITIONED_ARTIFACT_METADATA\"\n",
    "client = storage.Client(project=\"cal-itp-data-infra\")\n",
    "fs, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d24c4-761c-4f66-8e60-277b01f87987",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_v2_bucket = storage.Bucket(client=client, name=\"test-calitp-gtfs-schedule-raw\")\n",
    "old_v2_outcomes = list(old_v2_bucket.list_blobs(prefix=\"download_schedule_feed_results\"))\n",
    "\n",
    "new_v2_bucket = storage.Bucket(client=client, name=os.environ['CALITP_BUCKET__GTFS_SCHEDULE_RAW'].replace('gs://', ''))\n",
    "new_v2_outcomes = list(new_v2_bucket.list_blobs(prefix=\"download_schedule_feed_results\"))\n",
    "\n",
    "len(old_v2_outcomes), old_v2_outcomes[-1], len(new_v2_outcomes), new_v2_outcomes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a3fd0-2395-4480-bfb1-a18f4421764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_results_blob = old_v2_outcomes[0]\n",
    "one_results_blob.bucket.name, one_results_blob.path, one_results_blob.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521519e-46fc-4531-abf9-1494e6a481a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "Counter(\n",
    "    str(json.loads(file.metadata[PARTITIONED_ARTIFACT_METADATA_KEY]).keys())\n",
    "    for file in old_v2_outcomes\n",
    "), Counter(\n",
    "    str(json.loads(file.metadata[PARTITIONED_ARTIFACT_METADATA_KEY]).keys())\n",
    "    for file in new_v2_outcomes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f4169-154f-4a41-b2dc-a95cf9e42fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(f\"gs://{one_results_blob.bucket.name}/{one_results_blob.name}\") as f:\n",
    "    results = [json.loads(line) for line in f.readlines()]\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd779c-761a-4ccb-918e-cc1f619ef3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import ClassVar, List, Optional\n",
    "\n",
    "import pendulum\n",
    "from calitp.storage import (\n",
    "    GTFSDownloadConfig,\n",
    "    GTFSFeedType,\n",
    "    GTFSScheduleFeedExtract,\n",
    "    PartitionedGCSArtifact,\n",
    "    ProcessingOutcome,\n",
    ")\n",
    "from google.cloud import storage\n",
    "from pydantic import HttpUrl, parse_obj_as\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = \"gs://test-calitp-gtfs-schedule-raw-v2\"\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome]\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()\n",
    "\n",
    "def handle_one_results_blob(results_blob, dry_run=True):\n",
    "    to_copy = []\n",
    "    new_outcomes = []\n",
    "    with fs.open(f\"gs://{results_blob.bucket.name}/{results_blob.name}\") as f:\n",
    "        old_outcomes = [json.loads(line) for line in f.readlines()]\n",
    "    for result in tqdm(old_outcomes):\n",
    "        old_extract = result[\"extract\"]\n",
    "        old_airtable_record = result[\"airtable_record\"]\n",
    "        \n",
    "        # we always need to be able to get a download config, even if there's no \"real\" file underlying it\n",
    "        new_config = GTFSDownloadConfig(\n",
    "            extracted_at=None,\n",
    "            name=old_airtable_record[\"name\"],\n",
    "            url=parse_obj_as(HttpUrl, old_airtable_record[\"pipeline_url\"]),\n",
    "            feed_type=GTFSFeedType.schedule,\n",
    "            schedule_url_for_validation=None,\n",
    "            auth_query_params={},\n",
    "            auth_headers={},\n",
    "        )\n",
    "        \n",
    "        # if we had a failure, there is no extract to copy\n",
    "        if not result['extract']:\n",
    "            new_outcomes.append(\n",
    "                GTFSDownloadOutcome(\n",
    "                    success=result[\"success\"],\n",
    "                    exception=Exception(result[\"exception\"]),\n",
    "                    config=new_config,\n",
    "                    extract=None,\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "        \n",
    "        # if we were successful, we should have a \n",
    "        ts = pendulum.parse(old_extract[\"ts\"])\n",
    "        dt = ts.date()\n",
    "        base64_url = base64.urlsafe_b64encode(\n",
    "            old_airtable_record[\"pipeline_url\"].encode()\n",
    "        ).decode()\n",
    "\n",
    "        # the old v2 files have url then ts, but we will be swapping them\n",
    "        old_blob_key = f\"schedule/dt={dt.to_date_string()}/base64_url={base64_url}/ts={ts.to_iso8601_string()}/{old_extract['filename']}\"\n",
    "        old_blob = results_blob.bucket.get_blob(old_blob_key)\n",
    "        if old_blob is None:\n",
    "            #print(new_config.url)\n",
    "            new_outcomes.append(\n",
    "                GTFSDownloadOutcome(\n",
    "                    success=False,\n",
    "                    exception=Exception(\"blob missing during backfill operation\"),\n",
    "                    config=new_config,\n",
    "                    extract=None,\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        assert old_blob.metadata[PARTITIONED_ARTIFACT_METADATA_KEY] == json.dumps(old_extract)\n",
    "\n",
    "        new_extract = GTFSScheduleFeedExtract(\n",
    "            filename=old_extract['filename'],\n",
    "            config=new_config,\n",
    "            response_code=old_extract[\"response_code\"],\n",
    "            response_headers=old_extract[\"response_headers\"],\n",
    "            ts=old_extract[\"ts\"],\n",
    "        )\n",
    "        \n",
    "        to_copy.append((f\"gs://{old_blob.bucket.name}/{old_blob.name}\", new_extract))\n",
    "        new_outcomes.append(\n",
    "            GTFSDownloadOutcome(\n",
    "                success=result[\"success\"],\n",
    "                exception=result[\"exception\"],\n",
    "                config=new_config,\n",
    "                extract=new_extract,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # only copy results once successful\n",
    "    old_metadata = json.loads(results_blob.metadata[PARTITIONED_ARTIFACT_METADATA_KEY])\n",
    "    new_result = DownloadFeedsResult(\n",
    "        ts=pendulum.parse(old_metadata['ts']),\n",
    "        end=pendulum.parse(old_metadata['end']),\n",
    "        outcomes=new_outcomes,\n",
    "        filename=old_metadata['filename'],\n",
    "    )\n",
    "    assert len(old_outcomes) == len(new_outcomes)\n",
    "    assert len(to_copy) == len([result for result in new_result.outcomes if result.success])\n",
    "    if not dry_run:\n",
    "        pass\n",
    "    return new_result, to_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbadae-d9f5-4a92-953b-62aab9ddf732",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result, to_copy = handle_one_results_blob(results_blob=one_results_blob, dest_bucket=None)\n",
    "len(to_copy), to_copy[0], new_result.bucket, new_result.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec509d-bda7-4533-bed9-07a8699b17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_copy[0][0], to_copy[0][1].path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e13c9-d97e-407d-ab31-dff90476dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "to_copies = []\n",
    "\n",
    "for results_blob in tqdm(old_v2_outcomes):\n",
    "    new_result, to_copy = handle_one_results_blob(results_blob=one_results_blob)\n",
    "    results.append(new_result)\n",
    "    to_copies.extend(to_copy)\n",
    "len(results), len(to_copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c5248-8864-4c2d-92ed-e42428929f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(first_files, columns=[\"blob\"])\n",
    "df[\"metadata\"] = df.blob.apply(\n",
    "    lambda b: str(json.loads(b.metadata[PARTITIONED_ARTIFACT_METADATA_KEY]).keys())\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820cf4a-e7aa-47a2-9519-f34efc7f2f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3d401-a10d-4be9-b4da-007c6b5178c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from typing import Dict\n",
    "\n",
    "import pendulum\n",
    "from calitp.storage import (\n",
    "    AirtableGTFSDataExtract,\n",
    "    AirtableGTFSDataRecord,\n",
    "    get_latest_file,\n",
    ")\n",
    "\n",
    "# os.environ[\"CALITP_BUCKET__AIRTABLE\"] =\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def get_airtable_gtfs_records_for_day(\n",
    "    dt: pendulum.Date,\n",
    ") -> Dict[str, AirtableGTFSDataRecord]:\n",
    "    file = get_latest_file(\n",
    "        # AirtableGTFSDataExtract.bucket,\n",
    "        \"gs://test-calitp-airtable\",\n",
    "        AirtableGTFSDataExtract.table,\n",
    "        prefix_partitions={\n",
    "            \"dt\": dt,\n",
    "        },\n",
    "        partition_types={\n",
    "            \"ts\": pendulum.DateTime,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    with get_fs().open(file.name, \"rb\") as f:\n",
    "        content = gzip.decompress(f.read())\n",
    "    records = [\n",
    "        AirtableGTFSDataRecord(**json.loads(row))\n",
    "        for row in content.decode().splitlines()\n",
    "    ]\n",
    "\n",
    "    return {record.id: record for record in records}\n",
    "\n",
    "\n",
    "def schedule_record_ids_for_validation_to_actual_url(dt, record_ids):\n",
    "    if not record_ids:\n",
    "        return None\n",
    "    if len(record_ids) == 1:\n",
    "        records = get_airtable_gtfs_records_for_day(dt)\n",
    "        return records[record_ids[0]].uri\n",
    "    raise RuntimeError\n",
    "\n",
    "\n",
    "len(get_airtable_gtfs_records_for_day(pendulum.Date(2022, 8, 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c1932-036d-4423-b57e-76d924401f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
