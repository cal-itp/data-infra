{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21efbc-f3bd-4278-b356-994b2ddc77e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pendulum if we don't have it for some reason; this may be unnecessary now\n",
    "%pip install pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ac1b1-4492-4db0-a318-754e3d906bcf",
   "metadata": {},
   "source": [
    "# Classes & config\n",
    "classes from https://github.com/cal-itp/data-infra/blob/airtable-extracted-ts/airflow/dags/download_gtfs_schedule_v2/download_schedule_feeds.py\n",
    "\n",
    "This notebook was used to backfill data in the v2 pipeline by applying a new version of metadata (download config rather than airtable record) and flipping the order of the base64url and ts partitions. We'd changed the download job to use the same ts for all files in a job and decided to use dt/ts/base64url as the partition order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141de325-e20a-47d5-ac21-7c4470276224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration / setup\n",
    "import os\n",
    "os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"] = \"test-calitp-gtfs-schedule-raw-v2\"\n",
    "OLD_SCHEDULE_RAW_BUCKET = \"test-calitp-gtfs-schedule-raw\"\n",
    "JSONL_EXTENSION = \".jsonl\"\n",
    "\n",
    "from calitp.storage import get_fs\n",
    "fs = get_fs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d687ba-7e51-48d8-83c7-84f89ee0b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract, ProcessingOutcome, PartitionedGCSArtifact\n",
    "from typing import Optional, ClassVar, List\n",
    "from pydantic import validator\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = \"test-calitp-gtfs-schedule-raw-v2\"\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome]\n",
    "\n",
    "    @validator(\"ts\")\n",
    "    def coerce_ts(cls, v):\n",
    "        return pendulum.instance(v)\n",
    "    @validator(\"end\")\n",
    "    def coerce_end(cls, v):\n",
    "        return pendulum.instance(v)\n",
    "    \n",
    "    @validator(\"filename\", allow_reuse=True)\n",
    "    def is_jsonl(cls, v):\n",
    "        assert v.endswith(JSONL_EXTENSION)\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()\n",
    "\n",
    "    @property\n",
    "    def successes(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if outcome.success]\n",
    "\n",
    "    @property\n",
    "    def failures(self) -> List[GTFSDownloadOutcome]:\n",
    "        return [outcome for outcome in self.outcomes if not outcome.success]\n",
    "\n",
    "    # TODO: I dislike having to exclude the records here\n",
    "    #   I need to figure out the best way to have a single type represent the \"metadata\" of\n",
    "    #   the content as well as the content itself\n",
    "    def save(self, fs):\n",
    "        self.save_content(\n",
    "            fs=fs,\n",
    "            content=\"\\n\".join(o.json() for o in self.outcomes).encode(),\n",
    "            exclude={\"outcomes\"},\n",
    "        )\n",
    "d = json.loads(fs.getxattr(path=\"gs://test-calitp-gtfs-schedule-raw/download_schedule_feed_results/dt=2022-09-01/ts=2022-09-01T00:00:26.548709+00:00/results.jsonl\", attr=\"PARTITIONED_ARTIFACT_METADATA\"))\n",
    "DownloadFeedsResult(outcomes=[], **d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e571987-2749-4ddf-910b-a548a0abcaee",
   "metadata": {},
   "source": [
    "# Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6763d9c-bbac-47d3-9ec5-ee119905cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "from tqdm.notebook import tqdm\n",
    "results_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/download_schedule_feed_results/', recursive=True)\n",
    "results_files = [file for file in results_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "\n",
    "data_files = fs.expand_path(f'gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/', recursive=True)\n",
    "data_files = [file for file in data_files if fs.stat(file)[\"type\"] != \"directory\"]\n",
    "\n",
    "results_paths = [(path, *path.split(\"/\")) for path in results_files]\n",
    "results_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea8da-d5ee-4d07-ae4b-22e24a045e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import pendulum\n",
    "import json\n",
    "from datetime import datetime\n",
    "from calitp.storage import GTFSDownloadConfig, GTFSScheduleFeedExtract\n",
    "\n",
    "# record raw files to be moved\n",
    "moves = []\n",
    "\n",
    "# record results objects to be saved\n",
    "results_to_save = []\n",
    "\n",
    "# invalid records\n",
    "drops = {}\n",
    "\n",
    "pbar = tqdm(results_paths)\n",
    "for og_path, bucket, table, dt, ts, filename in pbar:\n",
    "    pbar.set_description(f\"processing {dt}\")\n",
    "    \n",
    "    # checks \n",
    "    assert table == \"download_schedule_feed_results\"\n",
    "    pdt = pendulum.parse(dt.replace(\"dt=\", \"\"), exact=True)\n",
    "    assert isinstance(pdt, pendulum.Date)\n",
    "    pts = pendulum.parse(ts.replace(\"ts=\", \"\"), exact=True)\n",
    "    assert isinstance(pts, pendulum.DateTime)\n",
    "    \n",
    "    with fs.open(og_path) as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    new_outcomes = []\n",
    "    \n",
    "    new_drops = []\n",
    "    \n",
    "    content_pbar = tqdm(content.decode().splitlines(), leave=False)\n",
    "    # load outcomes rows just as json, converting exception strings to exceptions\n",
    "    for row in content_pbar:\n",
    "        content_json = json.loads(row)\n",
    "        content_json[\"exception\"] = Exception(content_json[\"exception\"]) if content_json[\"exception\"] else None\n",
    "        \n",
    "        # if success, we have a file \n",
    "        if content_json[\"success\"]:\n",
    "            \n",
    "            extract_pts = pendulum.parse(content_json[\"extract\"][\"ts\"], exact=True)\n",
    "            assert isinstance(extract_pts, pendulum.DateTime)\n",
    "            \n",
    "            assert content_json[\"extract\"][\"config\"][\"uri\"] == content_json[\"airtable_record\"][\"uri\"], f'extract uri {content_json[\"extract\"][\"config\"][\"uri\"]} differs from airtable record uri: {content_json[\"airtable_record\"][\"uri\"]}'\n",
    "\n",
    "            old_config = content_json[\"extract\"].pop(\"config\")\n",
    "            old_extract = content_json[\"extract\"]\n",
    "            base64url = base64.urlsafe_b64encode(old_config[\"uri\"].encode()).decode() \n",
    "            old_extract_path = f'gs://{OLD_SCHEDULE_RAW_BUCKET}/schedule/dt={extract_pts.to_date_string()}/base64_url={base64url}/ts={extract_pts.to_iso8601_string()}/{old_extract[\"filename\"]}'\n",
    "            \n",
    "            assert fs.exists(old_extract_path), f\"error: {old_extract_path} does not exist; mismatch between outcomes and actual files\"\n",
    "\n",
    "            new_config = GTFSDownloadConfig(\n",
    "                    name = old_config.get(\"name\"),\n",
    "                    auth_query_params = {old_config.get(\"authorization_url_parameter_name\"): old_config.get(\"url_secret_key_name\")} if old_config.get(\"authorization_url_parameter_name\") else {},\n",
    "                    auth_headers = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")} if old_config.get(\"authorization_header_parameter_name\") else {},\n",
    "                    feed_type = old_config.get(\"data\"),\n",
    "                    url = old_config[\"uri\"],\n",
    "                    schedule_url_for_validation = None\n",
    "                    )\n",
    "\n",
    "            new_extract = GTFSScheduleFeedExtract(\n",
    "                config = new_config,\n",
    "                **old_extract\n",
    "                )\n",
    "\n",
    "            new_outcome = GTFSDownloadOutcome(\n",
    "                success = content_json[\"success\"],\n",
    "                exception = content_json[\"exception\"],\n",
    "                config = new_config,\n",
    "                extract = new_extract\n",
    "            )\n",
    "            \n",
    "            new_outcomes.append(new_outcome)\n",
    "            moves.append((old_extract_path, f'gs://{new_extract.path}', new_extract))\n",
    "            \n",
    "        else:\n",
    "            old_config = content_json[\"airtable_record\"]\n",
    "            \n",
    "            try:\n",
    "                new_config = GTFSDownloadConfig(\n",
    "                    name = old_config.get(\"name\"),\n",
    "                    auth_query_params = {old_config.get(\"authorization_url_parameter_name\"): old_config.get(\"url_secret_key_name\")} if old_config.get(\"authorization_url_parameter_name\") else {},\n",
    "                    auth_headers = {old_config.get(\"authorization_header_parameter_name\"): old_config.get(\"header_secret_key_name\")} if old_config.get(\"authorization_header_parameter_name\") else {},\n",
    "                    feed_type = old_config.get(\"data\"),\n",
    "                    url = old_config[\"uri\"],\n",
    "                    schedule_url_for_validation = None\n",
    "                    )\n",
    "                \n",
    "                new_outcome = GTFSDownloadOutcome(\n",
    "                    success = content_json[\"success\"],\n",
    "                    exception = content_json[\"exception\"],\n",
    "                    config = new_config\n",
    "                    )\n",
    "                \n",
    "                new_outcomes.append(new_outcome)\n",
    "                \n",
    "            except Exception as e:\n",
    "                new_drops.append(content_json) \n",
    "            \n",
    "    \n",
    "    len_outcomes = len(new_outcomes)\n",
    "    len_drops = len(new_drops)\n",
    "    len_content = len(content.decode().splitlines())\n",
    "    assert len_outcomes + len_drops == len_content, f\"got {len_outcomes} outcomes and {len_drops} drops from {len_content} input records\"\n",
    "    new_results = DownloadFeedsResult(outcomes=new_outcomes, **json.loads(fs.getxattr(path=f\"gs://{og_path}\", attr=\"PARTITIONED_ARTIFACT_METADATA\")))\n",
    "    if len_drops:\n",
    "        drops[pdt] = new_drops\n",
    "    results_to_save.append(new_results)\n",
    "\n",
    "len_moves = len(moves)\n",
    "len_data_files = len(data_files)\n",
    "assert len_data_files == len_data_files, f\"got {len_moves} from {len_data_files}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3804f6-fdbb-421e-b76f-7d1201d7b0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moves[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a415db2-6682-44d7-a36f-e2bf26821b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_to_save[0].ts, len(results_to_save[0].outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b4fd1-e63b-4fbd-aa4f-c3d506be4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moves), len(results_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f52bc-465f-4088-b9d9-5a29607852f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for og_path, new_path, new_extract in tqdm(moves):\n",
    "    #fs.cp(og_path, new_path)\n",
    "    #fs.setxattrs(path=new_path, PARTITIONED_ARTIFACT_METADATA=new_extract.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03756990-89b3-4679-9880-a9d7a30a8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in tqdm(results_to_save):\n",
    "    result.save(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec25a7-b552-4b47-9d27-52ceb3e65eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
