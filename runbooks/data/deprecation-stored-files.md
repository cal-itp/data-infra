# Deleting / deprecating files in Google Cloud Storage

Occasionally, we want to assess our Google Cloud Storage buckets for outdatedness and deprecate buckets that we have no use for. Please follow the steps outlined here when performing a deletion or deprecation.

1. In the Google Cloud Console Metrics Explorer, identify GCS buckets that have not recently had data written to them or read from them - [this query](https://console.cloud.google.com/monitoring/metrics-explorer;duration=P84D?pageState=%7B%22domainObjectDeprecationId%22:%22D20E3E2D-1786-4C36-988D-09C4EB19587E%22,%22title%22:%22Untitled%22,%22xyChart%22:%7B%22constantLines%22:%5B%5D,%22dataSets%22:%5B%7B%22plotType%22:%22LINE%22,%22targetAxis%22:%22Y1%22,%22timeSeriesFilter%22:%7B%22aggregations%22:%5B%7B%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22groupByFields%22:%5B%22metric.label.%5C%22method%5C%22%22,%22resource.label.%5C%22bucket_name%5C%22%22%5D,%22perSeriesAligner%22:%22ALIGN_RATE%22%7D%5D,%22apiSource%22:%22DEFAULT_CLOUD%22,%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22filter%22:%22metric.type%3D%5C%22storage.googleapis.com%2Fapi%2Frequest_count%5C%22%20resource.type%3D%5C%22gcs_bucket%5C%22%20resource.label.%5C%22bucket_name%5C%22%3D%5C%22calitp-gtfs-rt-raw-v2%5C%22%22,%22groupByFields%22:%5B%22metric.label.%5C%22method%5C%22%22,%22resource.label.%5C%22bucket_name%5C%22%22%5D,%22minAlignmentPeriod%22:%2260s%22,%22perSeriesAligner%22:%22ALIGN_RATE%22%7D%7D%5D,%22options%22:%7B%22mode%22:%22COLOR%22%7D,%22y1Axis%22:%7B%22label%22:%22%22,%22scale%22:%22LINEAR%22%7D%7D%7D&project=cal-itp-data-infra) produces a visualization of recent activity for objects within a given bucket (the targeted bucket is set in the "Filters" section, with the calitp-gtfs-rt-raw-v2 bucket provided as an example). For any bucket without recent ReadObject or WriteObject activity, proceed to the next steps.

2. Among buckets not recently modified (more than ~12 weeks since the last update), there are two general categories:
    * Buckets used for testing, prefixed with "test-", generally correspond to infrequent tests of Airflow jobs and other scripts that take place during new feature development. The buckets themselves should generally remain in existence (unless the corresponding job/script is no longer actively used in production), but any objects they contain from previous rounds of testing can be deleted. Note that deletion of objects from some test buckets may introduce a later need to place raw artifacts inside those buckets in order to test parsing and validation tasks.
    * All other buckets are deprecation candidates, but should be treated with greater care, utilizing the remaining steps of this guide.

3. For the non-test buckets that constitute the deprecation candidate list, the path forward relies on investigation of internal project configuration and conversation with data stakeholders. Some data may need to be kept in place because it is frequently accessed despite being infrequently updated (NTD data or static website assets, for instance). Some data may need to be cold-stored rather than deleted outright because it represents raw data collected once that can't otherwise be recovered, or to conform with regulatory requirements, or to provide a window for future research access. Each of the following steps should be taken to determine which path to take:
    * Search the source code of the [data-infra repository](https://github.com/cal-itp/data-infra), the [data-analyses repository](https://github.com/cal-itp/data-analyses), and the [reports repository](https://github.com/cal-itp/reports) for the name of the bucket, as well as the environment variables [set in Cloud Composer](https://console.cloud.google.com/composer/environments/detail/us-west2/calitp-airflow2-prod/variables?project=cal-itp-data-infra). If you find it referenced anywhere, investigate whether the reference is in active use. For an extra step of safety, you could also search the entire Cal-ITP GitHub organization's source code via GitHub's web user interface.
        * Note: [External tables](https://cloud.google.com/bigquery/docs/external-tables) in BigQuery, created from GCS objects via [our `create_external_tables` DAG](https://o1d2fa0877cf3fb10p-tp.appspot.com/dags/create_external_tables/grid) in Airflow, do not produce read or write data that shows up in the GCS request count metric we used in step one. If you find a reference to a deprecation candidate bucket within the [`create_external_tables` subfolder](https://github.com/cal-itp/data-infra/tree/main/airflow/dags/create_external_tables) of the data-infra repository, you should check [BigQuery audit logs](https://cloud.google.com/bigquery/docs/reference/auditlogs/#data_access_data_access) to see whether people are querying the external tables that rely on the deprecation candidate bucket (and if so, eliminate it from the deprecation list).
    * Post in `#data-warehouse-devs` and any other relevant channels in Slack (this may vary by domain; for example, if investigating a bucket related to GTFS quality, you may post in `#gtfs-quality`). Ask whether anybody knows of ongoing use of the bucket(s) in question. If there are identifiable stakeholders who aren't active in Slack, like external research partners, reach out to them directly.

4. For each bucket that hasn't been removed from the deprecation list via the investigation in the last step, create a new bucket named "[EXISTING BUCKET NAME]-deprecated" with the default storage class set to Archive (setting the storage class is one of the bucket creation steps) and follow [these steps](https://cloud.google.com/storage/docs/moving-buckets#permissions-console) to transfer the original bucket contents into the newly created bucket(s). Delete the original bucket(s), inform stakeholders about the newly deprecated buckets via `#data-warehouse-devs` and other relevant channels, and monitor for two weeks for any new code or process breakages related to the deletion of the old buckets.

5. After two weeks is up, take the most relevant option of the following two:
    * For buckets that must be retained because they represent raw data that can't otherwise be recovered in the future, or for regulatory reasons, or because of potential for future research/analysis access, place a small README inside the "-deprecated" bucket explaining its deprecation. Note that future access to objects in this Archive class bucket will incur higher costs than access to objects in the standard storage class - the Archive storage class is intended for data that will not be accessed frequently, and is cost-optimized for lower costs of storage and higher costs of access.
    * For buckets that do not need to be kept long term, like out-of-use transformations of raw data that can be recreated if necessary from the corresponding raw data, simply delete the "-deprecated" bucket.
