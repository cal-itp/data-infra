{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b00d77be-a1ff-4b7e-871f-439809803451",
   "metadata": {},
   "source": [
    "# Real historical backfill aka agencies.yml v1 to post-GTFSDownloadConfig v2\n",
    "\n",
    "[The actual PR](https://github.com/cal-itp/data-infra/pull/2033)\n",
    "\n",
    "I created a new bucket (`gs://gtfs-schedule-backfill-test`) and used [data transfer](https://console.cloud.google.com/transfer/jobs/transferJobs%2F5240403197777129047/runs?project=cal-itp-data-infra) to copy just schedule production data (`gs://gtfs-data/schedule`) into the bucket. I then identified a date range to process by looking for our first `status.csv` and the oldest data in `\"gs://calitp-gtfs-schedule-raw-v2\"` which gives us a range of 2021-04-15 to 2022-09-14 inclusive.\n",
    "\n",
    "The outcome and result classes from https://github.com/cal-itp/data-infra/blob/8dadfb31c9546af1f2ed20d19fd1d2d06c66282d/airflow/dags/download_gtfs_schedule_v2/download_schedule_feeds.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4937d16-a44d-4ad5-b300-a22309dc6652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# may be necessary depending on version of the jupyter singleuser image you are using\n",
    "# we need the config type with the computed field\n",
    "!pip install calitp==2022.12.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e2980-2cee-49d1-8b59-2af67811da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pendulum\n",
    "from google.cloud import storage\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"] = \"gs://test-calitp-gtfs-schedule-raw-v2\"\n",
    "SOURCE_BUCKET_PATH = \"gs://gtfs-schedule-backfill-test/schedule/\"\n",
    "PARTITIONED_ARTIFACT_METADATA_KEY = \"PARTITIONED_ARTIFACT_METADATA\"\n",
    "first_date = pendulum.parse(\"2021-04-15\", exact=True)\n",
    "first_date_v2 = pendulum.parse(\"2022-09-14\", exact=True)\n",
    "\n",
    "# import this after environ change\n",
    "from calitp_data.storage import get_fs\n",
    "\n",
    "fs = get_fs()\n",
    "client = storage.Client(project=\"cal-itp-data-infra\")\n",
    "fs, client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeeb7bc-6a5b-496a-8a42-32de7e3b4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = []\n",
    "for d in fs.ls(SOURCE_BUCKET_PATH):\n",
    "    if \"T00:00:00\" not in d:\n",
    "        # skip some old ones that don't have midnight execution times, we probably shouldn't trust them?\n",
    "        continue\n",
    "    ts = pendulum.parse(d.split(\"/\")[-1])\n",
    "    if first_date <= ts.date() and ts.date() <= first_date_v2:\n",
    "        folders.append(d)\n",
    "len(folders), folders[0], folders[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f7d728-320a-40f7-97b9-99798d171037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import ClassVar, List, Optional\n",
    "\n",
    "import pendulum\n",
    "from calitp_data_infra.storage import (\n",
    "    GTFSDownloadConfig,\n",
    "    GTFSFeedType,\n",
    "    GTFSScheduleFeedExtract,\n",
    "    PartitionedGCSArtifact,\n",
    "    ProcessingOutcome,\n",
    ")\n",
    "from google.cloud import storage\n",
    "from pydantic import HttpUrl, parse_obj_as, validator\n",
    "\n",
    "\n",
    "class GTFSDownloadOutcome(ProcessingOutcome):\n",
    "    config: GTFSDownloadConfig\n",
    "    extract: Optional[GTFSScheduleFeedExtract]\n",
    "    backfilled: bool = False\n",
    "\n",
    "\n",
    "class DownloadFeedsResult(PartitionedGCSArtifact):\n",
    "    bucket: ClassVar[str] = os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"]\n",
    "    table: ClassVar[str] = \"download_schedule_feed_results\"\n",
    "    partition_names: ClassVar[List[str]] = [\"dt\", \"ts\"]\n",
    "    ts: pendulum.DateTime\n",
    "    end: pendulum.DateTime\n",
    "    outcomes: List[GTFSDownloadOutcome] = []\n",
    "    backfilled: bool = False\n",
    "\n",
    "    # @validator(\"backfilled\", allow_reuse=True)\n",
    "    # def everything_backfilled(cls, v, values):\n",
    "    #    outcomes_backfilled = set(outcome.backfilled for outcome in values[\"outcomes\"])\n",
    "    #    assert {v} == outcomes_backfilled\n",
    "    #    return v\n",
    "\n",
    "    @property\n",
    "    def dt(self) -> pendulum.Date:\n",
    "        return self.ts.date()\n",
    "\n",
    "    def save(self, fs):\n",
    "        self.save_content(\n",
    "            fs=fs,\n",
    "            content=\"\\n\".join(o.json() for o in self.outcomes).encode(),\n",
    "            exclude={\"outcomes\"},\n",
    "        )\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c62ad-8332-46cd-9fa7-c8d3dbe8dc58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import Future, ThreadPoolExecutor, as_completed\n",
    "from pprint import pprint\n",
    "from zipfile import ZipFile, ZipInfo\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel, ValidationError, parse_obj_as\n",
    "\n",
    "# jinja pattern for removing the auth query parameter if it exists\n",
    "jinja_pattern = r\"(?<=\\?)(?:api_key|token)=[\\w-]+&?\"\n",
    "\n",
    "\n",
    "class SkipUrl(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ZipTask(BaseModel):\n",
    "    outcome: GTFSDownloadOutcome\n",
    "    gcs_dir: Optional[str]\n",
    "    files: Optional[Dict[str, pendulum.DateTime]]\n",
    "\n",
    "\n",
    "def feed_folder_to_zip_task(\n",
    "    folder, feed, zip_one_feed_fs=None, pbar=None\n",
    ") -> ZipTask:\n",
    "    zip_one_feed_fs = get_fs() if not zip_one_feed_fs else zip_one_feed_fs\n",
    "    feed_key = f\"{feed['itp_id']}_{feed['url_number']}\"\n",
    "    url = re.sub(jinja_pattern, \"\", feed[\"gtfs_schedule_url\"]).rstrip(\"?\")\n",
    "\n",
    "    assert (\n",
    "        url\n",
    "        and \"token\" not in url\n",
    "        and \"api_key\" not in url\n",
    "        and not url.endswith(\"?\")\n",
    "        and not url.endswith(\"&\")\n",
    "        and \"?&\" not in url\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        validated_url = parse_obj_as(HttpUrl, url)\n",
    "    except ValidationError:\n",
    "        if url.startswith(\"http://.232\"):\n",
    "            raise SkipUrl\n",
    "        raise\n",
    "\n",
    "    config = GTFSDownloadConfig(\n",
    "        extracted_at=None,\n",
    "        name=feed[\"agency_name\"],\n",
    "        url=validated_url,\n",
    "        feed_type=GTFSFeedType.schedule,\n",
    "        schedule_url_for_validation=None,\n",
    "        auth_query_params={},\n",
    "        auth_headers={},\n",
    "        computed=True,\n",
    "    )\n",
    "\n",
    "    if feed[\"status\"] != \"success\":\n",
    "        return ZipTask(\n",
    "            outcome=GTFSDownloadOutcome(\n",
    "                success=False,\n",
    "                exception=Exception(feed[\"status\"]),\n",
    "                config=config,\n",
    "                extract=None,\n",
    "                backfilled=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    feed_folder = f\"{folder}/{feed_key}\"\n",
    "    files_to_timestamps = {}\n",
    "\n",
    "    for current_dir, sub_dirs, files in zip_one_feed_fs.walk(feed_folder):\n",
    "        # skip our processed subdir\n",
    "        # skip situations where we have a weird duplicate subdir\n",
    "        if (\n",
    "            current_dir.endswith(\"processed\")\n",
    "            or \"__MACOSX\" in current_dir\n",
    "            or f\"{feed_key}/{feed_key}\" in current_dir\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith(\"validation.json\"):\n",
    "                continue\n",
    "\n",
    "            filename = (\n",
    "                file\n",
    "                if current_dir.endswith(feed_key)\n",
    "                else f\"{os.path.basename(current_dir)}/{file}\"\n",
    "            )\n",
    "            files_to_timestamps[filename] = (\n",
    "                pendulum.parse(\n",
    "                    zip_one_feed_fs.stat(f\"gs://{current_dir}/{file}\")[\"customTime\"],\n",
    "                    exact=True,\n",
    "                )\n",
    "                .in_tz(\"Etc/UTC\")\n",
    "                .replace(microsecond=0)\n",
    "            )\n",
    "\n",
    "    if not files_to_timestamps:\n",
    "        print(feed_key, to_walk, list(zip_one_feed_fs.walk(f\"{folder}/{feed_key}\")))\n",
    "        raise RuntimeError\n",
    "\n",
    "    first_ts = min(files_to_timestamps.values())\n",
    "    last_ts = max(files_to_timestamps.values())\n",
    "\n",
    "    if (last_ts - first_ts).total_seconds() > 600:\n",
    "        print(\"got weirdly long extract: \", (last_ts - first_ts), to_walk)\n",
    "\n",
    "    extract = GTFSScheduleFeedExtract(\n",
    "        ts=first_ts,\n",
    "        config=config,\n",
    "        response_code=200,  # this is somewhat assumed\n",
    "        filename=\"reconstructed.zip\",\n",
    "        reconstructed=True,\n",
    "    )\n",
    "\n",
    "    assert \"+00:00/base64_url\" in extract.path\n",
    "\n",
    "    return ZipTask(\n",
    "        outcome=GTFSDownloadOutcome(\n",
    "            success=True,\n",
    "            exception=None,\n",
    "            config=config,\n",
    "            extract=extract,\n",
    "            backfilled=True,\n",
    "        ),\n",
    "        gcs_dir=feed_folder,\n",
    "        files=files_to_timestamps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568dce50-0058-4fd4-b7fb-e2a2af93de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of a single folder; can verify that we get a correct output task\n",
    "# We want to check that the file names and timestamps are appropriate, and the config has the correct URL etc.\n",
    "\n",
    "# eastern-sierra-transit-authority,Eastern Sierra Transit Authority,99,http://data.trilliumtransit.com/gtfs/easternsierra-ca-us/easternsierra-ca-us.zip,0,success\n",
    "# 232 has a subfolder\n",
    "zt = feed_folder_to_zip_task(\n",
    "    folder=f\"{SOURCE_BUCKET_PATH}2021-05-21T00:00:00+00:00\",\n",
    "    feed={\n",
    "        \"itp_id\": \"324\",\n",
    "        \"url_number\": \"0\",\n",
    "        \"status\": \"success\",\n",
    "        \"agency_name\": \"Eastern Sierra Transit Authority\",\n",
    "        \"gtfs_schedule_url\": \"http://data.trilliumtransit.com/gtfs/easternsierra-ca-us/easternsierra-ca-us.zip\",\n",
    "    },\n",
    ")\n",
    "zt.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8257e6-5a6c-419d-87ce-682cc637af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def get_zip_tasks_from_folder(\n",
    "    folder, handle_one_folder_fs=None, pool=None, top_pbar=None, i=None,\n",
    ") -> Tuple[DownloadFeedsResult, List[ZipTask]]:\n",
    "    fs = handle_one_folder_fs if handle_one_folder_fs else get_fs()\n",
    "\n",
    "    with fs.open(f\"gs://{folder}/status.csv\", \"r\") as f:\n",
    "        rows = list(csv.DictReader(f))\n",
    "\n",
    "    deduplicated = {feed[\"gtfs_schedule_url\"]: feed for feed in rows}\n",
    "    \n",
    "    tasks = []\n",
    "    skipped = 0\n",
    "\n",
    "    pbar = tqdm(total=len(deduplicated), desc=f\"{i} {folder}\", leave=False)\n",
    "    futures = {\n",
    "        pool.submit(\n",
    "            feed_folder_to_zip_task,\n",
    "            folder=folder,\n",
    "            feed=feed,\n",
    "            pbar=pbar,\n",
    "            zip_one_feed_fs=fs,\n",
    "        ): feed\n",
    "        for feed in deduplicated.values()\n",
    "    }\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        feed = futures[future]\n",
    "        pbar.update()\n",
    "        try:\n",
    "            tasks.append(future.result())\n",
    "        except SkipUrl:\n",
    "            skipped += 1\n",
    "        except Exception:\n",
    "            print(feed)\n",
    "            raise\n",
    "\n",
    "    assert len(deduplicated) == (len(tasks) + skipped)\n",
    "    outcomes = [task.outcome for task in tasks]\n",
    "    ts = min(outcome.extract.ts for outcome in outcomes if outcome.extract)\n",
    "    end = max(outcome.extract.ts for outcome in outcomes if outcome.extract)\n",
    "    \n",
    "    for outcome in outcomes:\n",
    "        if outcome.extract:\n",
    "            outcome.extract.ts = ts\n",
    "\n",
    "    if (end - ts).total_seconds() > 600:\n",
    "        print(\"got weirdly long extract: \", (end - ts), folder)\n",
    "\n",
    "    result = DownloadFeedsResult(\n",
    "        ts=ts,\n",
    "        end=ts,\n",
    "        outcomes=outcomes,\n",
    "        filename=\"results.jsonl\",\n",
    "        backfilled=True,\n",
    "    )\n",
    "\n",
    "    assert result.path.startswith(\n",
    "        f'{os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"]}/download_schedule_feed_results'\n",
    "    ) and result.path.endswith(\"+00:00/results.jsonl\")\n",
    "    assert all(\n",
    "        task.outcome.extract.path.startswith(\n",
    "            f'{os.environ[\"CALITP_BUCKET__GTFS_SCHEDULE_RAW\"]}/schedule'\n",
    "        )\n",
    "        and \"+00:00/base64_url\" in task.outcome.extract.path\n",
    "        for task in tasks\n",
    "        if task.outcome.extract\n",
    "    )\n",
    "    # if top_pbar:\n",
    "    #     top_pbar.write(\n",
    "    #         f\"i:{i} rows:{len(rows)} dedup:{len(deduplicated)} outs:{len(outcomes)} skip:{skipped} result: {result.path}\"\n",
    "    #     )\n",
    "    return result, tasks\n",
    "\n",
    "\n",
    "# result, outcomes_extracts_bytes = handle_one_folder(\"gtfs-schedule-backfill-test/schedule/2021-04-17T00:00:00+00:00\", threads=12)\n",
    "# result.path, extracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e363f2e-a292-4f42-b9bd-ed947387beb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test just one\n",
    "with ThreadPoolExecutor(max_workers=32) as pool:\n",
    "    result, tasks = get_zip_tasks_from_folder(\n",
    "        folder=folders[0],\n",
    "        handle_one_folder_fs=None,\n",
    "        pool=pool,\n",
    "        top_pbar=None,\n",
    "        i=None,\n",
    "    )\n",
    "result.path, set(task.outcome.extract.ts for task in tasks if task.outcome.extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4d42c-743d-42e8-9813-48e98dd68efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this to generate and save all tasks and results files\n",
    "import pickle\n",
    "fs = get_fs()\n",
    "with ThreadPoolExecutor(max_workers=64) as pool:\n",
    "    all_results = []\n",
    "    all_tasks = []\n",
    "    # fs = get_fs()\n",
    "    # If the kernal dies or you otherwise need to restart from a point, you can\n",
    "    # re-run this cell with a portion of folders\n",
    "    # for example folders[340:]\n",
    "    # folders_pbar = tqdm(folders[410:])\n",
    "    # folders_pbar = tqdm(list(random.sample(folders[:10], 20)))\n",
    "    folders_pbar = tqdm(folders)\n",
    "    for i, folder in enumerate(folders_pbar):\n",
    "        try:\n",
    "            result, tasks = get_zip_tasks_from_folder(\n",
    "                folder=folder,\n",
    "                handle_one_folder_fs=fs,\n",
    "                pool=pool,\n",
    "                top_pbar=folders_pbar,\n",
    "                i=i,\n",
    "            )\n",
    "            assert {result.ts} == set(task.outcome.extract.ts for task in tasks if task.outcome.extract)\n",
    "            all_results.append(result)\n",
    "            all_tasks.extend([task for task in tasks if task.outcome.extract])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"unable to find status in {folder}\")\n",
    "        # break\n",
    "\n",
    "with open(\"results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(all_results, f)\n",
    "with open(\"tasks.pickle\", \"wb\") as f:\n",
    "    pickle.dump(all_tasks, f)\n",
    "len(all_results), len(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066774aa-71d7-425e-8e52-a8f07a08f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be run to load pickled results\n",
    "import pickle\n",
    "with open(\"results.pickle\", \"rb\") as f:\n",
    "    all_results = pickle.load(f)\n",
    "with open(\"tasks.pickle\", \"rb\") as f:\n",
    "    all_tasks = pickle.load(f)\n",
    "len(all_results), len(all_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfca021-d69c-481e-b5e7-7b1fa4916456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "def execute_zip_task(task: ZipTask, dry_run=True, execute_zip_task_fs=None):\n",
    "    fs = execute_zip_task_fs if execute_zip_task_fs else get_fs()\n",
    "    assert isinstance(task, ZipTask)\n",
    "    bytesio = io.BytesIO()\n",
    "    with ZipFile(bytesio, \"w\") as zipf:\n",
    "        for file, creation_ts in task.files.items():\n",
    "            file_gcs_path = f\"{task.gcs_dir}/{file}\"\n",
    "            zipinfo = ZipInfo(\n",
    "                filename=file,\n",
    "                date_time=(\n",
    "                    creation_ts.year,\n",
    "                    creation_ts.month,\n",
    "                    creation_ts.day,\n",
    "                    creation_ts.hour,\n",
    "                    creation_ts.minute,\n",
    "                    creation_ts.second,\n",
    "                ),\n",
    "            )\n",
    "            zipf.writestr(zipinfo, fs.cat(file_gcs_path))\n",
    "    bytesio.seek(0)\n",
    "    zipfile_bytes = bytesio.read()\n",
    "    if dry_run:\n",
    "        print(\n",
    "            f\"DRY RUN: would be saving {humanize.naturalsize(len(zipfile_bytes))} to {task.outcome.extract.path}\"\n",
    "        )\n",
    "    else:\n",
    "        task.outcome.extract.save_content(fs=fs, content=zipfile_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257702af-7589-4677-a048-abe433348976",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks[0].gcs_dir, all_tasks[0].files, all_tasks[0].outcome.extract.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635d55a-688a-4ba0-8eff-6eb9f67b8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "assert len(all_tasks) == len(set(task.outcome.extract.path for task in all_tasks))\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e897039-703c-45a9-81e7-13ce76a5d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually perform the zips; chunk so we can restart from a given chunk even though the underlying tasks are not executed in order\n",
    "\n",
    "import random\n",
    "\n",
    "fses = [get_fs() for _ in range(10)]\n",
    "chunks = list(enumerate(grouper(1000, all_tasks)))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=32) as pool:\n",
    "    for i, chunk in tqdm(chunks):\n",
    "        pbar = tqdm(total=len(chunk), desc=f\"chunk {i}\", leave=False)\n",
    "        futures = {\n",
    "            pool.submit(\n",
    "                execute_zip_task,\n",
    "                task=task,\n",
    "                dry_run=False,\n",
    "                execute_zip_task_fs=random.choice(fses),\n",
    "            ): task\n",
    "            for task in chunk if task\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            pbar.update()\n",
    "            try:\n",
    "                future.result()\n",
    "            except:\n",
    "                print(futures[future])\n",
    "                raise\n",
    "        del pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6056f08-1ed1-45d8-bb33-85b4edc5f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in tqdm(all_results):\n",
    "    result.save(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec964cc-3667-472a-b667-4d565d462dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e577d02-354e-4978-9ce9-fa6ff19bd2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
