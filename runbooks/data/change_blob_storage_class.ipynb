{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc712f-d33f-4265-a47e-7b2dc4ac1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from calitp_data.storage import get_fs\n",
    "from tqdm.notebook import tqdm\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734280d2-2b1a-4270-9f9f-56cd7295fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_bucket = 'gtfs-data-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db2bc2-28bd-4503-a71f-2ffccb5059bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_large_subpaths = [f'{read_from_bucket}/rt', f'{read_from_bucket}/schedule', f'{read_from_bucket}/rt-fixed-timestamp']\n",
    "\n",
    "dt_subpaths = [f'{read_from_bucket}/rt', f'{read_from_bucket}/schedule']\n",
    "fixed_timestamp_subpaths = [f'{read_from_bucket}/rt-fixed-timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79b890-306a-465b-adea-b749c1b6cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subpath_blobs(read_from_bucket, run_larger_subpaths=True, run_smaller_subpaths=True):\n",
    "    \n",
    "    fs = get_fs()\n",
    "    subpaths = fs.ls('gs://' + read_from_bucket)\n",
    "    \n",
    "    smaller_subpaths = [subpath for subpath in subpaths if subpath not in all_large_subpaths]\n",
    "    \n",
    "    if run_smaller_subpaths:\n",
    "        for subpath in tqdm(smaller_subpaths):\n",
    "            print(f'starting smaller subpaths')\n",
    "            print(f'starting {subpath}')\n",
    "\n",
    "            # create new blob name from subpath for blob write\n",
    "            new_blob_name = subpath.split('/')[1]\n",
    "            if new_blob_name.find('.'):\n",
    "                 new_blob_name = new_blob_name.split('.')[0]\n",
    "\n",
    "            # open connection to storage client\n",
    "            storage_client = storage.Client()\n",
    "\n",
    "            # name write-to bucket\n",
    "            write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "            \n",
    "            # declare write-to bucket\n",
    "            bucket = storage_client.bucket(write_to_bucket)\n",
    "            \n",
    "            # name/ create blob to write to\n",
    "            blob = bucket.blob(f'{new_blob_name}.txt')\n",
    "\n",
    "            # create prefix out of subpath to read bucket\n",
    "            # this is different than new_blob_name above - it preserves the file file extension\n",
    "            prefix = subpath.split('/')[1]\n",
    "\n",
    "            # list contents of read bucket in gcs file object\n",
    "            print('listing contents in read-from bucket')\n",
    "            file_object = list(storage_client.list_blobs(read_from_bucket, prefix=prefix))\n",
    "\n",
    "            # open up new blob in write mode\n",
    "            with blob.open('w') as f:\n",
    "                \n",
    "                print('writing results to files')\n",
    "                for result in tqdm(file_object):\n",
    "                    f.write(result.name + '\\n')\n",
    "                f.close()\n",
    "            \n",
    "            print(f'finished with {subpath}')\n",
    "\n",
    "        print('finished with smaller subpaths')\n",
    "        \n",
    "    if run_larger_subpaths:\n",
    "        for subpath in all_large_subpaths:\n",
    "            print(f'starting {subpath}')\n",
    "            shortened_subpath = subpath.split('/')[1]\n",
    "            \n",
    "            print(f'walking top subpaths')\n",
    "            top_subpaths = list(fs.walk(f'gs://{read_from_bucket}/{shortened_subpath}/', maxdepth=0)) # , recursive=True\n",
    "            just_subpath_names = top_subpaths[0][1]\n",
    "            \n",
    "            split_subpath_names = []\n",
    "            \n",
    "            print(f'finding unique dates')\n",
    "            if subpath in dt_subpaths:\n",
    "                for sp in just_subpath_names:\n",
    "                    sp = sp.split('T')[0]\n",
    "                    split_subpath_names.append(sp)\n",
    "            \n",
    "            if subpath in fixed_timestamp_subpaths:\n",
    "                split_subpath_names = just_subpath_names\n",
    "\n",
    "            unique_split_subpath_names = unique(split_subpath_names)\n",
    "    \n",
    "            print(f'writing unique subpath names')\n",
    "            # open file in write mode\n",
    "            with open(f'top_{shortened_subpath}_directories.txt', 'w') as fp:\n",
    "                for item in tqdm(unique_split_subpath_names):\n",
    "                    # write each item on a new line\n",
    "                    fp.write(item + '\\n')\n",
    "                fp.close()\n",
    "                print(f'finished writing top_{shortened_subpath}_directories.txt')\n",
    "\n",
    "            print(f'reading unique subpath names')\n",
    "            with open(f'top_{shortened_subpath}_directories.txt', 'r') as nfp:\n",
    "                unique_split_subpath_names_dirty = list(nfp)\n",
    "                unique_split_subpath_names_clean = []\n",
    "                for ud in unique_split_subpath_names_dirty:\n",
    "                    ud = ud.replace('\\n', '')\n",
    "                    unique_split_subpath_names_clean.append(ud)\n",
    "\n",
    "            for unique_subpath in tqdm(unique_split_subpath_names_clean):\n",
    "\n",
    "                # open connection to storage client\n",
    "                storage_client = storage.Client()\n",
    "\n",
    "                # write-to bucket name\n",
    "                write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "                # identify bucket to write to\n",
    "                bucket = storage_client.bucket(write_to_bucket)\n",
    "\n",
    "                # create blob to write to\n",
    "                blob = bucket.blob(f'{shortened_subpath}/{unique_subpath}.txt')\n",
    "\n",
    "                # list contents of read bucket in gcs file object\n",
    "                all_date_files = list(storage_client.list_blobs(read_from_bucket, prefix = f'{shortened_subpath}/' + unique_subpath.lstrip().rstrip()))\n",
    "                all_target_files = []\n",
    "                for adf in all_date_files:\n",
    "                    target_file_name = adf.name\n",
    "                    all_target_files.append(target_file_name)\n",
    "\n",
    "                # open up new blob in write mode\n",
    "                with blob.open('w') as f:\n",
    "\n",
    "                    for result in tqdm(all_target_files):\n",
    "                        f.write(result + '\\n')\n",
    "                    f.close()\n",
    "            print(f'finished with {subpath}')\n",
    "        \n",
    "        print('finished with larger subpaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a9fdd-2898-4710-8141-8431876a8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list):\n",
    " \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    " \n",
    "    # traverse for all elements\n",
    "    for x in list:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e36ac1-3ea1-481d-9077-6e1b6f15165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subpath_blobs(read_from_bucket, run_larger_subpaths=False) #run_smaller_subpaths=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18228d7c-e523-4dfa-a644-97e93ceda8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6e6a5-5a07-4b13-99f0-a9cb6b237700",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_blobs = [f'cold-storage-outputs-{read_from_bucket}/rt-fixed-timestamp' , f'cold-storage-outputs-{read_from_bucket}/rt', f'cold-storage-outputs-{read_from_bucket}/schedule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e73421-830c-424e-826d-e25a9a99c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_file_storage_class(read_from_bucket, run_larger_subpaths=True, run_smaller_subpaths=True):\n",
    "    '''Change the default storage class of the blob'''\n",
    "\n",
    "    fs = get_fs()\n",
    "    subpaths = fs.ls(f'gs://cold-storage-outputs-{read_from_bucket}')\n",
    "    smaller_blobs = [subpath for subpath in subpaths if subpath not in larger_blobs]\n",
    "    clean_smaller_blobs = [smaller_blob.split('/')[1] for smaller_blob in smaller_blobs]\n",
    "    clean_larger_blobs = [file_blob.split('/')[1] for file_blob in larger_blobs]\n",
    "    \n",
    "    if run_smaller_subpaths:\n",
    "        for file_blob in tqdm(clean_smaller_blobs):\n",
    "            \n",
    "            storage_client = storage.Client()\n",
    "\n",
    "            # declare bucket to read the list of files created from gtfs-data\n",
    "            bucket = storage_client.bucket(f'cold-storage-outputs-{read_from_bucket}')\n",
    "\n",
    "            # name/ create blob to write to\n",
    "            blob = bucket.blob(file_blob)\n",
    "\n",
    "            with blob.open(\"r\") as f:\n",
    "                class_change_list = []\n",
    "                for each_blob in f:\n",
    "                    each_blob = each_blob.replace(\"\\n\", \"\")\n",
    "                    class_change_list.append(each_blob)\n",
    "            \n",
    "            for item in class_change_list:\n",
    "\n",
    "                bucket_name = read_from_bucket\n",
    "                blob_name = item\n",
    "                bucket = storage_client.bucket(bucket_name)\n",
    "                blob = bucket.blob(blob_name)\n",
    "                generation_match_precondition = None\n",
    "\n",
    "                # Optional: set a generation-match precondition to avoid potential race\n",
    "                # conditions and data corruptions. The request is aborted if the\n",
    "                # object's generation number does not match your precondition.\n",
    "                blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n",
    "                generation_match_precondition = blob.generation\n",
    "\n",
    "                blob.update_storage_class('ARCHIVE', if_generation_match=generation_match_precondition)\n",
    "\n",
    "                print(\n",
    "                    'Blob {} in bucket {} had its storage class set to {}'.format(\n",
    "                        blob_name,\n",
    "                        bucket_name,\n",
    "                        blob.storage_class\n",
    "                    )\n",
    "                )\n",
    "                return blob\n",
    "            \n",
    "        storage_client.close()\n",
    "\n",
    "\n",
    "    if run_larger_subpaths:\n",
    "\n",
    "        for file_blob in tqdm(clean_larger_blobs):\n",
    "            \n",
    "            storage_client = storage.Client()\n",
    "\n",
    "            # declare bucket to read the list of files created from gtfs-data\n",
    "            bucket = storage_client.bucket(f'cold-storage-outputs-{read_from_bucket}')\n",
    "\n",
    "            # name/ create blob to write to\n",
    "            blob = bucket.blob(file_blob)\n",
    "            \n",
    "            print(f'walking top subpaths')\n",
    "            top_subpaths = list(fs.walk(f'gs://cold-storage-outputs-{read_from_bucket}/{file_blob}/', maxdepth=0)) # , recursive=True\n",
    "\n",
    "            just_subpath_names = top_subpaths[0][2]\n",
    "            \n",
    "            for subpath in just_subpath_names:\n",
    "                # declare bucket to read the list of files created from gtfs-data\n",
    "                bucket_name = f'cold-storage-outputs-{read_from_bucket}'\n",
    "\n",
    "                bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "                # name/ create blob to write to\n",
    "                blob = bucket.blob(f'{file_blob}/{subpath}')\n",
    "                \n",
    "                blob.reload()\n",
    "                \n",
    "                class_change_list = []\n",
    "                with blob.open(\"r\") as f:\n",
    "                    for each_blob in f:\n",
    "                        each_blob = each_blob.replace(\"\\n\", \"\")\n",
    "                        class_change_list.append(each_blob)\n",
    "\n",
    "                for item in class_change_list:\n",
    "\n",
    "                    bucket_name = read_from_bucket\n",
    "                    blob_name = item\n",
    "                    bucket = storage_client.bucket(bucket_name)\n",
    "                    blob = bucket.blob(blob_name)\n",
    "\n",
    "                    generation_match_precondition = None\n",
    "\n",
    "                    # Optional: set a generation-match precondition to avoid potential race\n",
    "                    # conditions and data corruptions. The request is aborted if the\n",
    "                    # object's generation number does not match your precondition.\n",
    "                    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n",
    "                    generation_match_precondition = blob.generation\n",
    "\n",
    "                    blob.update_storage_class('ARCHIVE', if_generation_match=generation_match_precondition)\n",
    "\n",
    "                    print(\n",
    "                        'Blob {} in bucket {} had its storage class set to {}'.format(\n",
    "                            blob_name,\n",
    "                            bucket_name,\n",
    "                            blob.storage_class\n",
    "                        )\n",
    "                    )\n",
    "                    return blob\n",
    "\n",
    "            storage_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd6cb3-ac2d-4cb2-ac6f-6dd7212e9e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_file_storage_class(read_from_bucket, run_smaller_subpaths=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3707bbb-d749-4e68-8a62-da0cec916440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to check the storage class type\n",
    "# for each_blob in class_change_list[:3]:\n",
    "#     bucket = storage_client.bucket('gtfs-data-test')\n",
    "#     blob = bucket.blob(each_blob)\n",
    "#     blob.reload()\n",
    "#     print(blob.storage_class)\n",
    "# storage_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b43fe8-d9a7-4ac8-b35a-729398e2a3a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce266ae-d4c6-49de-8f3d-ae466ff42f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use this if need to create a new bucket\n",
    "\n",
    "#storage_client = storage.Client()\n",
    "\n",
    "#storage_client.create_bucket('cold-storage-outputs-gtfs-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94f9d4-1da2-4c92-8298-e9f41a2aa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use below for memory monitoring\n",
    "\n",
    "# # starting the monitoring\n",
    "# tracemalloc.start()\n",
    "\n",
    "# # displaying the memory\n",
    "# print(tracemalloc.get_traced_memory())\n",
    " \n",
    "# # stopping the library\n",
    "# tracemalloc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
