{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc712f-d33f-4265-a47e-7b2dc4ac1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from calitp_data.storage import get_fs\n",
    "from tqdm.notebook import tqdm\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734280d2-2b1a-4270-9f9f-56cd7295fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_bucket = 'gtfs-data-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caab6d9-9d0a-4894-8c1a-d7a2572dc95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list):\n",
    " \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    " \n",
    "    # traverse for all elements\n",
    "    for x in list:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79b890-306a-465b-adea-b749c1b6cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subpath_blobs(read_from_bucket, run_larger_subpaths=True, run_smaller_subpaths=True):\n",
    "    \n",
    "    fs = get_fs()\n",
    "    subpaths = fs.ls('gs://' + read_from_bucket)\n",
    "    \n",
    "    all_large_subpaths = [f'{read_from_bucket}/rt', f'{read_from_bucket}/schedule', f'{read_from_bucket}/rt-fixed-timestamp']\n",
    "    \n",
    "    dt_subpaths = [f'{read_from_bucket}/rt', f'{read_from_bucket}/schedule']\n",
    "    fixed_timestamp_subpaths = [f'{read_from_bucket}/rt-fixed-timestamp']\n",
    "    \n",
    "    smaller_subpaths = [subpath for subpath in subpaths if subpath not in all_large_subpaths]\n",
    "    \n",
    "    if run_smaller_subpaths:\n",
    "        for subpath in tqdm(smaller_subpaths):\n",
    "            print(f'starting smaller subpaths')\n",
    "            print(f'starting {subpath}')\n",
    "\n",
    "            # create new blob name from subpath for blob write\n",
    "            new_blob_name = subpath.split('/')[1]\n",
    "            if new_blob_name.find('.'):\n",
    "                 new_blob_name = new_blob_name.split('.')[0]\n",
    "\n",
    "            # open connection to storage client\n",
    "            storage_client = storage.Client()\n",
    "\n",
    "            # name write-to bucket\n",
    "            write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "            \n",
    "            # declare write-to bucket\n",
    "            bucket = storage_client.bucket(write_to_bucket)\n",
    "            \n",
    "            # name/ create blob to write to\n",
    "            blob = bucket.blob(f'{new_blob_name}.txt')\n",
    "\n",
    "            # create prefix out of subpath to read bucket\n",
    "            # this is different than new_blob_name above - it preserves the file file extension\n",
    "            prefix = subpath.split('/')[1]\n",
    "\n",
    "            # list contents of read bucket in gcs file object\n",
    "            print('listing contents in read-from bucket')\n",
    "            file_object = list(storage_client.list_blobs(read_from_bucket, prefix=prefix))\n",
    "\n",
    "            # open up new blob in write mode\n",
    "            with blob.open('w') as f:\n",
    "                \n",
    "                print('writing results to files')\n",
    "                for result in tqdm(file_object):\n",
    "                    f.write(result.name + '\\n')\n",
    "                f.close()\n",
    "            \n",
    "            print(f'finished with {subpath}')\n",
    "\n",
    "        print('finished with smaller subpaths')\n",
    "        \n",
    "    \n",
    "    if run_larger_subpaths:\n",
    "        for subpath in all_large_subpaths:\n",
    "            print(f'starting {subpath}')\n",
    "            shortened_subpath = subpath.split('/')[1]\n",
    "            \n",
    "            print(f'walking top subpaths')\n",
    "            top_subpaths = list(fs.walk(f'gs://{read_from_bucket}/{shortened_subpath}/', maxdepth=0)) # , recursive=True\n",
    "            just_subpath_names = top_subpaths[0][1]\n",
    "            \n",
    "            split_subpath_names = []\n",
    "            \n",
    "            print(f'finding unique dates')\n",
    "            if subpath in dt_subpaths:\n",
    "                for sp in just_subpath_names:\n",
    "                    sp = sp.split('T')[0]\n",
    "                    split_subpath_names.append(sp)\n",
    "            \n",
    "            if subpath in fixed_timestamp_subpaths:\n",
    "                split_subpath_names = just_subpath_names\n",
    "\n",
    "            unique_split_subpath_names = unique(split_subpath_names)\n",
    "    \n",
    "            print(f'writing unique subpath names')\n",
    "            # open file in write mode\n",
    "            with open(f'top_{shortened_subpath}_directories.txt', 'w') as fp:\n",
    "                for item in tqdm(unique_split_subpath_names):\n",
    "                    # write each item on a new line\n",
    "                    fp.write(item + '\\n')\n",
    "                fp.close()\n",
    "                print(f'finished writing top_{shortened_subpath}_directories.txt')\n",
    "\n",
    "            print(f'unique subpath names')\n",
    "            with open(f'top_{shortened_subpath}_directories.txt', 'r') as nfp:\n",
    "                unique_split_subpath_names_dirty = list(nfp)\n",
    "                unique_split_subpath_names_clean = []\n",
    "                for ud in unique_split_subpath_names_dirty:\n",
    "                    ud = ud.replace('\\n', '')\n",
    "                    unique_split_subpath_names_clean.append(ud)\n",
    "\n",
    "            for unique_subpath in tqdm(unique_split_subpath_names_clean):\n",
    "\n",
    "                # open connection to storage client\n",
    "                storage_client = storage.Client()\n",
    "\n",
    "\n",
    "                # write-to bucket name\n",
    "                write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "                # identify bucket to write to\n",
    "                bucket = storage_client.bucket(write_to_bucket)\n",
    "\n",
    "                # create blob to write to\n",
    "                blob = bucket.blob(f'{shortened_subpath}/{unique_subpath}.txt')\n",
    "\n",
    "\n",
    "                # list contents of read bucket in gcs file object\n",
    "                all_date_files = list(storage_client.list_blobs(read_from_bucket, prefix = f'{shortened_subpath}/' + unique_subpath.lstrip().rstrip()))\n",
    "                all_target_files = []\n",
    "                for adf in all_date_files:\n",
    "                    target_file_name = adf.name\n",
    "                    all_target_files.append(target_file_name)\n",
    "\n",
    "                # open up new blob in write mode\n",
    "                with blob.open('w') as f:\n",
    "\n",
    "                    for result in tqdm(all_target_files):\n",
    "                        f.write(result + '\\n')\n",
    "                    f.close()\n",
    "            print(f'finished with {subpath}')\n",
    "        \n",
    "\n",
    "        print('finished with larger subpaths')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e36ac1-3ea1-481d-9077-6e1b6f15165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subpath_blobs(read_from_bucket, run_smaller_subpaths=False) #run_larger_subpaths=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca39afa-4477-4a3f-b2d7-061f0416962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_file_storage_class(bucket_name, blob_name):\n",
    "    '''Change the default storage class of the blob'''\n",
    "    # bucket_name = 'your-bucket-name'\n",
    "    # blob_name = 'your-object-name'\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    generation_match_precondition = None\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race\n",
    "    # conditions and data corruptions. The request is aborted if the\n",
    "    # object's generation number does not match your precondition.\n",
    "    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n",
    "    generation_match_precondition = blob.generation\n",
    "\n",
    "    blob.update_storage_class('NEARLINE', if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        'Blob {} in bucket {} had its storage class set to {}'.format(\n",
    "            blob_name,\n",
    "            bucket_name,\n",
    "            blob.storage_class\n",
    "        )\n",
    "    )\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab51bb6-7203-430e-8354-bfacbac29eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_file_storage_class(bucket_name, blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f86fb-ca45-429a-a48e-86fbcfec61b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce266ae-d4c6-49de-8f3d-ae466ff42f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use this if need to create a new bucket\n",
    "\n",
    "#storage_client = storage.Client()\n",
    "\n",
    "#storage_client.create_bucket('cold-storage-outputs-gtfs-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94f9d4-1da2-4c92-8298-e9f41a2aa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use below for memory monitoring\n",
    "\n",
    "# starting the monitoring\n",
    "tracemalloc.start()\n",
    "\n",
    "# displaying the memory\n",
    "print(tracemalloc.get_traced_memory())\n",
    " \n",
    "# stopping the library\n",
    "tracemalloc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
