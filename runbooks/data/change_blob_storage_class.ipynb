{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f691870-a77d-4d33-8561-3a3b9384fa1c",
   "metadata": {},
   "source": [
    "TO-DO:\n",
    "- parallelize listing of files\n",
    "- introduce skipping the listing blobs already listed\n",
    "\n",
    "For Monday:\n",
    "- hardcode the skipping of folders that I have already changed the storage class of (for archive storageclass)\n",
    "(can add a skip to the function that does the change which says if not already archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc712f-d33f-4265-a47e-7b2dc4ac1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from calitp_data.storage import get_fs\n",
    "from tqdm.notebook import tqdm\n",
    "from google.cloud import storage\n",
    "\n",
    "# parallelization\n",
    "import concurrent.futures\n",
    "\n",
    "# testing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734280d2-2b1a-4270-9f9f-56cd7295fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_from_bucket = 'gtfs-data-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db2bc2-28bd-4503-a71f-2ffccb5059bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_large_subpaths = [f'{read_from_bucket}/rt', f'{read_from_bucket}/schedule', f'{read_from_bucket}/rt-fixed-timestamp']\n",
    "\n",
    "dt_subpaths = [f'{read_from_bucket}/rt', f'{read_from_bucket}/schedule']\n",
    "fixed_timestamp_subpaths = [f'{read_from_bucket}/rt-fixed-timestamp']\n",
    "\n",
    "larger_blobs = [f'cold-storage-outputs-{read_from_bucket}/rt-fixed-timestamp' , f'cold-storage-outputs-{read_from_bucket}/rt', f'cold-storage-outputs-{read_from_bucket}/schedule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c54509-a4bb-4690-95a8-d87c7858670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list):\n",
    " \n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    " \n",
    "    # traverse for all elements\n",
    "    for x in list:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c4fc8-720b-4f37-a8dd-03c95e644e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contents of read bucket in gcs file object\n",
    "def list_bucket_contents(file_list):\n",
    "    \n",
    "    all_target_files = []\n",
    "    \n",
    "    for item in file_list:\n",
    "        target_file_name = item.name\n",
    "        all_target_files.append(target_file_name)\n",
    "        \n",
    "    # create blob to write to\n",
    "    blob = bucket.blob(f'{shortened_subpath}/{unique_subpath}.txt')\n",
    "        \n",
    "    # open up new blob in write mode\n",
    "    with blob.open('w') as f:\n",
    "\n",
    "        for result in tqdm(all_target_files):\n",
    "            f.write(result + '\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0784a-ef6b-486d-903e-304c6fc96a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_list_bucket_contents(date_files_list, progress: bool = True):\n",
    "    pbar = tqdm(total=len(date_files_list)) if progress else None\n",
    "        \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        \n",
    "        # Start the load operations and mark each future with its URL                    \n",
    "        future_to_list_bucket_contents = {executor.submit(list_bucket_contents, date_files_list)}\n",
    "        ## future_to_list_bucket_contents = {executor.submit(list_bucket_contents, list_item): list_item for list_item in date_files_list}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_list_bucket_contents):\n",
    "            list_item = future_to_list_bucket_contents[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (list_item, exc))\n",
    "            \n",
    "            if pbar:\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d62572-6e60-4739-8929-0aeaba60b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subpath_blobs(read_from_bucket, run_larger_subpaths=True, run_smaller_subpaths=True):\n",
    "    \n",
    "    fs = get_fs()\n",
    "    subpaths = fs.ls('gs://' + read_from_bucket)\n",
    "    \n",
    "    smaller_subpaths = [subpath for subpath in subpaths if subpath not in all_large_subpaths]\n",
    "    \n",
    "    # name write-to bucket\n",
    "    write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "\n",
    "    # declare write-to bucket\n",
    "    bucket = storage_client.bucket(write_to_bucket)\n",
    "    \n",
    "    # open connection to storage client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    if run_smaller_subpaths:\n",
    "        for subpath in tqdm(smaller_subpaths):\n",
    "            print(f'starting smaller subpaths')\n",
    "            print(f'starting {subpath}')\n",
    "\n",
    "            # create new blob name from subpath for writing blob\n",
    "            new_blob_name = subpath.split('/')[1]\n",
    "            if new_blob_name.find('.'):\n",
    "                 new_blob_name = new_blob_name.split('.')[0]\n",
    "            \n",
    "            # name/ create blob to write to\n",
    "            blob = bucket.blob(f'{new_blob_name}.txt')\n",
    "\n",
    "            # create prefix out of subpath to read bucket\n",
    "            # this is different than new_blob_name above - it preserves the file extension\n",
    "            prefix = subpath.split('/')[1]\n",
    "\n",
    "            # list contents of read bucket in gcs file object\n",
    "            print('listing contents in read-from bucket')\n",
    "            file_object = list(storage_client.list_blobs(read_from_bucket, prefix=prefix))\n",
    "\n",
    "            # open up new blob in write mode\n",
    "            with blob.open('w') as f:\n",
    "                \n",
    "                print('writing results to files')\n",
    "                for result in tqdm(file_object):\n",
    "                    f.write(result.name + '\\n')\n",
    "                f.close()\n",
    "            \n",
    "            print(f'finished with {subpath}')\n",
    "\n",
    "        storage_client.close()\n",
    "        print('finished with smaller subpaths')\n",
    "        \n",
    "    if run_larger_subpaths:\n",
    "        for subpath in all_large_subpaths:\n",
    "            print(f'starting {subpath}')\n",
    "            shortened_subpath = subpath.split('/')[1]\n",
    "            \n",
    "            print(f'walking top subpaths')\n",
    "            top_subpaths = list(fs.walk(f'gs://{read_from_bucket}/{shortened_subpath}/', maxdepth=0)) # , recursive=True\n",
    "            just_subpath_names = top_subpaths[0][1]\n",
    "            \n",
    "            split_subpath_names = []\n",
    "            \n",
    "            print(f'finding unique dates')\n",
    "            if subpath in dt_subpaths:\n",
    "                for sp in just_subpath_names:\n",
    "                    sp = sp.split('T')[0]\n",
    "                    split_subpath_names.append(sp)\n",
    "            \n",
    "            if subpath in fixed_timestamp_subpaths:\n",
    "                split_subpath_names = just_subpath_names\n",
    "\n",
    "            unique_split_subpath_names = unique(split_subpath_names)\n",
    "    \n",
    "            print(f'writing unique subpath names')\n",
    "            # open file in write mode\n",
    "            with open(f'top_{shortened_subpath}_directories.txt', 'w') as fp:\n",
    "                for item in tqdm(unique_split_subpath_names):\n",
    "                    # write each item on a new line\n",
    "                    fp.write(item + '\\n')\n",
    "                fp.close()\n",
    "                \n",
    "                print(f'finished writing top_{shortened_subpath}_directories.txt')\n",
    "\n",
    "            print(f'reading unique subpath names')\n",
    "            with open(f'top_{shortened_subpath}_directories.txt', 'r') as nfp:\n",
    "                unique_split_subpath_names_dirty = list(nfp)\n",
    "                unique_split_subpath_names_clean = []\n",
    "                for ud in unique_split_subpath_names_dirty:\n",
    "                    ud = ud.replace('\\n', '')\n",
    "                    unique_split_subpath_names_clean.append(ud)\n",
    "            \n",
    "            for unique_subpath in tqdm(unique_split_subpath_names_clean):\n",
    "                \n",
    "                all_date_files = list(storage_client.list_blobs(read_from_bucket, prefix = f'{shortened_subpath}/' + unique_subpath.lstrip().rstrip()))\n",
    "\n",
    "                # parallelization\n",
    "                parallelize_list_bucket_contents(all_date_files)\n",
    "                                                 \n",
    "            print(f'finished with {subpath}')\n",
    "        \n",
    "        storage_client.close()\n",
    "        print('finished with larger subpaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79b890-306a-465b-adea-b749c1b6cdf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def list_subpath_blobs(read_from_bucket, run_larger_subpaths=True, run_smaller_subpaths=True):\n",
    "    \n",
    "#     fs = get_fs()\n",
    "#     subpaths = fs.ls('gs://' + read_from_bucket)\n",
    "    \n",
    "#     smaller_subpaths = [subpath for subpath in subpaths if subpath not in all_large_subpaths]\n",
    "    \n",
    "#     if run_smaller_subpaths:\n",
    "#         for subpath in tqdm(smaller_subpaths):\n",
    "#             print(f'starting smaller subpaths')\n",
    "#             print(f'starting {subpath}')\n",
    "\n",
    "#             # create new blob name from subpath for blob write\n",
    "#             new_blob_name = subpath.split('/')[1]\n",
    "#             if new_blob_name.find('.'):\n",
    "#                  new_blob_name = new_blob_name.split('.')[0]\n",
    "\n",
    "#             # open connection to storage client\n",
    "#             storage_client = storage.Client()\n",
    "\n",
    "#             # name write-to bucket\n",
    "#             write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "            \n",
    "#             # declare write-to bucket\n",
    "#             bucket = storage_client.bucket(write_to_bucket)\n",
    "            \n",
    "#             # name/ create blob to write to\n",
    "#             blob = bucket.blob(f'{new_blob_name}.txt')\n",
    "\n",
    "#             # create prefix out of subpath to read bucket\n",
    "#             # this is different than new_blob_name above - it preserves the file file extension\n",
    "#             prefix = subpath.split('/')[1]\n",
    "\n",
    "#             # list contents of read bucket in gcs file object\n",
    "#             print('listing contents in read-from bucket')\n",
    "#             file_object = list(storage_client.list_blobs(read_from_bucket, prefix=prefix))\n",
    "\n",
    "#             # open up new blob in write mode\n",
    "#             with blob.open('w') as f:\n",
    "                \n",
    "#                 print('writing results to files')\n",
    "#                 for result in tqdm(file_object):\n",
    "#                     f.write(result.name + '\\n')\n",
    "#                 f.close()\n",
    "            \n",
    "#             print(f'finished with {subpath}')\n",
    "\n",
    "#         print('finished with smaller subpaths')\n",
    "        \n",
    "#     if run_larger_subpaths:\n",
    "#         for subpath in all_large_subpaths:\n",
    "#             print(f'starting {subpath}')\n",
    "#             shortened_subpath = subpath.split('/')[1]\n",
    "            \n",
    "#             print(f'walking top subpaths')\n",
    "#             top_subpaths = list(fs.walk(f'gs://{read_from_bucket}/{shortened_subpath}/', maxdepth=0)) # , recursive=True\n",
    "#             just_subpath_names = top_subpaths[0][1]\n",
    "            \n",
    "#             split_subpath_names = []\n",
    "            \n",
    "#             print(f'finding unique dates')\n",
    "#             if subpath in dt_subpaths:\n",
    "#                 for sp in just_subpath_names:\n",
    "#                     sp = sp.split('T')[0]\n",
    "#                     split_subpath_names.append(sp)\n",
    "            \n",
    "#             if subpath in fixed_timestamp_subpaths:\n",
    "#                 split_subpath_names = just_subpath_names\n",
    "\n",
    "#             unique_split_subpath_names = unique(split_subpath_names)\n",
    "    \n",
    "#             print(f'writing unique subpath names')\n",
    "#             # open file in write mode\n",
    "#             with open(f'top_{shortened_subpath}_directories.txt', 'w') as fp:\n",
    "#                 for item in tqdm(unique_split_subpath_names):\n",
    "#                     # write each item on a new line\n",
    "#                     fp.write(item + '\\n')\n",
    "#                 fp.close()\n",
    "#                 print(f'finished writing top_{shortened_subpath}_directories.txt')\n",
    "\n",
    "#             print(f'reading unique subpath names')\n",
    "#             with open(f'top_{shortened_subpath}_directories.txt', 'r') as nfp:\n",
    "#                 unique_split_subpath_names_dirty = list(nfp)\n",
    "#                 unique_split_subpath_names_clean = []\n",
    "#                 for ud in unique_split_subpath_names_dirty:\n",
    "#                     ud = ud.replace('\\n', '')\n",
    "#                     unique_split_subpath_names_clean.append(ud)\n",
    "            \n",
    "#             ### parallelize this !!\n",
    "#             for unique_subpath in tqdm(unique_split_subpath_names_clean):\n",
    "\n",
    "#                 # open connection to storage client\n",
    "#                 storage_client = storage.Client()\n",
    "\n",
    "#                 # write-to bucket name\n",
    "#                 write_to_bucket = 'cold-storage-outputs-' + read_from_bucket\n",
    "#                 # identify bucket to write to\n",
    "#                 bucket = storage_client.bucket(write_to_bucket)\n",
    "\n",
    "#                 # create blob to write to\n",
    "#                 blob = bucket.blob(f'{shortened_subpath}/{unique_subpath}.txt')\n",
    "\n",
    "#                 # list contents of read bucket in gcs file object\n",
    "#                 all_date_files = list(storage_client.list_blobs(read_from_bucket, prefix = f'{shortened_subpath}/' + unique_subpath.lstrip().rstrip()))\n",
    "#                 all_target_files = []\n",
    "#                 for adf in all_date_files:\n",
    "#                     target_file_name = adf.name\n",
    "#                     all_target_files.append(target_file_name)\n",
    "\n",
    "#                 # open up new blob in write mode\n",
    "#                 with blob.open('w') as f:\n",
    "\n",
    "#                     for result in tqdm(all_target_files):\n",
    "#                         f.write(result + '\\n')\n",
    "#                     f.close()\n",
    "#             ### finish parallelization\n",
    "            \n",
    "#             print(f'finished with {subpath}')\n",
    "        \n",
    "#         print('finished with larger subpaths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e36ac1-3ea1-481d-9077-6e1b6f15165d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#list_subpath_blobs(read_from_bucket, run_larger_subpaths=False) #run_smaller_subpaths=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8125a1b-89fa-4564-bcfd-bccb10fa0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storage_class_changer(item, client):\n",
    "\n",
    "    bucket_name = read_from_bucket\n",
    "    blob_name = item\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    generation_match_precondition = None\n",
    "\n",
    "    blob.reload()  # Fetch blob metadata to use in generation_match_precondition.\n",
    "    generation_match_precondition = blob.generation\n",
    "\n",
    "    blob.update_storage_class('ARCHIVE', if_generation_match=generation_match_precondition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a145a-5b9e-41b8-9346-203bb3b0cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_storage_class_changer(change_list, progress: bool = True):\n",
    "    pbar = tqdm(total=len(change_list)) if progress else None\n",
    "    \n",
    "    client = storage.Client()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        # Start the load operations and mark each future with its URL                    \n",
    "        future_to_change_class = {executor.submit(storage_class_changer, list_item, client): list_item for list_item in change_list}\n",
    "        for future in concurrent.futures.as_completed(future_to_change_class):\n",
    "            list_item = future_to_change_class[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (list_item, exc))\n",
    "            \n",
    "            if pbar:\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e73421-830c-424e-826d-e25a9a99c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_file_storage_class(read_from_bucket, run_larger_subpaths=True, run_smaller_subpaths=True):\n",
    "    '''Change the default storage class of the blob'''\n",
    "\n",
    "    fs = get_fs()\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    subpaths = fs.ls(f'gs://cold-storage-outputs-{read_from_bucket}')\n",
    "    smaller_blobs = [subpath for subpath in subpaths if subpath not in larger_blobs]\n",
    "    clean_smaller_blobs = [smaller_blob.split('/')[1] for smaller_blob in smaller_blobs]\n",
    "    clean_larger_blobs = [file_blob.split('/')[1] for file_blob in larger_blobs]    \n",
    "    \n",
    "    if run_smaller_subpaths:\n",
    "        for file_blob in tqdm(clean_smaller_blobs):\n",
    "            if file_blob == 'vehicle_positions.txt':            \n",
    "        \n",
    "                # declare bucket to read the list of files created from gtfs-data\n",
    "                bucket = storage_client.bucket(f'cold-storage-outputs-{read_from_bucket}')\n",
    "\n",
    "                # name/ create blob to write to\n",
    "                blob = bucket.blob(file_blob)\n",
    "\n",
    "                with blob.open(\"r\") as f:\n",
    "                    class_change_list = f.read().splitlines() \n",
    "                \n",
    "                print(f'starting changing storage classes for {file_blob}')\n",
    "\n",
    "                parallelize_storage_class_changer(class_change_list)\n",
    "\n",
    "                print(f'finished changing storage classes for {file_blob}')\n",
    "            \n",
    "        storage_client.close()\n",
    "\n",
    "\n",
    "    if run_larger_subpaths:\n",
    "\n",
    "        for file_blob in tqdm(clean_larger_blobs):\n",
    "            \n",
    "            storage_client = storage.Client()\n",
    "\n",
    "            # declare bucket to read the list of files created from gtfs-data\n",
    "            bucket = storage_client.bucket(f'cold-storage-outputs-{read_from_bucket}')\n",
    "\n",
    "            # name/ create blob to write to\n",
    "            blob = bucket.blob(file_blob)\n",
    "            \n",
    "            print(f'walking top subpaths')\n",
    "            top_subpaths = list(fs.walk(f'gs://cold-storage-outputs-{read_from_bucket}/{file_blob}/', maxdepth=0)) # , recursive=True\n",
    "\n",
    "            just_subpath_names = top_subpaths[0][2]\n",
    "            \n",
    "            for subpath in just_subpath_names:\n",
    "                # declare bucket to read the list of files created from gtfs-data\n",
    "                bucket_name = f'cold-storage-outputs-{read_from_bucket}'\n",
    "\n",
    "                bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "                # name/ create blob to write to\n",
    "                blob = bucket.blob(f'{file_blob}/{subpath}')\n",
    "                \n",
    "                blob.reload()\n",
    "                \n",
    "                with blob.open(\"r\") as f:\n",
    "                    class_change_list = f.read().splitlines()\n",
    "                \n",
    "                parallelize_storage_class_changer(class_change_list)\n",
    "                \n",
    "            print(f'finished changing storage classes for {file_blob}')\n",
    "            \n",
    "            storage_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e4343-2b5c-4889-896c-8d3a92c0382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "change_file_storage_class(read_from_bucket, run_larger_subpaths=False)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b43fe8-d9a7-4ac8-b35a-729398e2a3a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce266ae-d4c6-49de-8f3d-ae466ff42f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use this if need to create a new bucket\n",
    "\n",
    "#storage_client = storage.Client()\n",
    "\n",
    "#storage_client.create_bucket('cold-storage-outputs-gtfs-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94f9d4-1da2-4c92-8298-e9f41a2aa721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use below for memory monitoring\n",
    "\n",
    "# # starting the monitoring\n",
    "# tracemalloc.start()\n",
    "\n",
    "# # displaying the memory\n",
    "# print(tracemalloc.get_traced_memory())\n",
    " \n",
    "# # stopping the library\n",
    "# tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e79ad2c-ac4d-4c62-8aed-22f64ef90416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## count number of subpaths -- remove\n",
    "# rt_list = []\n",
    "# f = open(\"top_rt_directories.txt\", \"r\")\n",
    "# for line in f:\n",
    "#     rt_list.append(line)\n",
    "# print(len(rt_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbdc52b-74df-47f3-88ab-1ba4ba2b0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to check the storage class type when needed\n",
    "# for each_blob in class_change_list[:3]:\n",
    "#     bucket = storage_client.bucket('gtfs-data-test')\n",
    "#     blob = bucket.blob(each_blob)\n",
    "#     blob.reload()\n",
    "#     print(blob.storage_class)\n",
    "# storage_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
