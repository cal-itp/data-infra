# Deleting / deprecating files in Google Cloud Storage

Occasionally, we want to assess our Google Cloud Storage buckets for outdatedness and deprecate buckets that we have no use for. Please follow the steps outlined here when performing a deletion or deprecation.

1. In the Google Cloud Console Metrics Explorer, identify GCS buckets that have not recently had data written to them or read from them - [this query](https://console.cloud.google.com/monitoring/metrics-explorer;duration=P84D?pageState=%7B%22domainObjectDeprecationId%22:%22D20E3E2D-1786-4C36-988D-09C4EB19587E%22,%22title%22:%22Untitled%22,%22xyChart%22:%7B%22constantLines%22:%5B%5D,%22dataSets%22:%5B%7B%22plotType%22:%22LINE%22,%22targetAxis%22:%22Y1%22,%22timeSeriesFilter%22:%7B%22aggregations%22:%5B%7B%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22groupByFields%22:%5B%22metric.label.%5C%22method%5C%22%22,%22resource.label.%5C%22bucket_name%5C%22%22%5D,%22perSeriesAligner%22:%22ALIGN_RATE%22%7D%5D,%22apiSource%22:%22DEFAULT_CLOUD%22,%22crossSeriesReducer%22:%22REDUCE_SUM%22,%22filter%22:%22metric.type%3D%5C%22storage.googleapis.com%2Fapi%2Frequest_count%5C%22%20resource.type%3D%5C%22gcs_bucket%5C%22%20resource.label.%5C%22bucket_name%5C%22%3D%5C%22calitp-gtfs-rt-raw-v2%5C%22%22,%22groupByFields%22:%5B%22metric.label.%5C%22method%5C%22%22,%22resource.label.%5C%22bucket_name%5C%22%22%5D,%22minAlignmentPeriod%22:%2260s%22,%22perSeriesAligner%22:%22ALIGN_RATE%22%7D%7D%5D,%22options%22:%7B%22mode%22:%22COLOR%22%7D,%22y1Axis%22:%7B%22label%22:%22%22,%22scale%22:%22LINEAR%22%7D%7D%7D&project=cal-itp-data-infra) produces a visualization of recent activity for objects within a given bucket (the targeted bucket is set in the "Filters" section, with the calitp-gtfs-rt-raw-v2 bucket provided as an example). For any bucket without recent ReadObject or WriteObject activity, proceed to the next steps.

2. Among buckets not recently modified (more than ~12 weeks since the last update), there are two general categories:

   - Buckets used for testing, prefixed with "test-", generally correspond to infrequent tests of Airflow jobs and other scripts that take place during new feature development. The buckets themselves should generally remain in existence (unless the corresponding job/script is no longer actively used in production), but any objects they contain from previous rounds of testing can be deleted. Note that deletion of objects from some test buckets may introduce a later need to place raw artifacts inside those buckets in order to test parsing and validation tasks.
   - All other buckets are deprecation candidates, but should be treated with greater care, utilizing the remaining steps of this guide.

3. For the non-test buckets that constitute the deprecation candidate list, the path forward relies on investigation of internal project configuration and conversation with data stakeholders. Some data may need to be retained because it is frequently accessed despite being infrequently updated (NTD data or static website assets, for instance). Some data may need to be retained rather than deleted because it represents raw data collected once that can't otherwise be recovered, or to conform with regulatory requirements, or to provide a window for future research access. Each of the following steps should be taken to determine which path to take:

   - Search the source code of the [data-infra repository](https://github.com/cal-itp/data-infra), the [data-analyses repository](https://github.com/cal-itp/data-analyses), and the [reports repository](https://github.com/cal-itp/reports) for the name of the bucket, as well as the environment variables [set in Cloud Composer](https://console.cloud.google/composer/environments/detail/us-west2/calitp-composer/variables?project=cal-itp-data-infra). If you find it referenced anywhere, investigate whether the reference is in active use. For an extra step of safety, you could also search the entire Cal-ITP GitHub organization's source code via GitHub's web user interface.
     - Note: [External tables](https://cloud.google.com/bigquery/docs/external-tables) in BigQuery, created from GCS objects via [our `create_external_tables` DAG](https://b15efed84aa34881b71da3b8fa87acd6-dot-us-west2.composer.googleusercontent.com/dags/create_external_tables/grid) in Airflow, do not produce read or write data that shows up in the GCS request count metric we used in step one. If you find a reference to a deprecation candidate bucket within the [`create_external_tables` subfolder](https://github.com/cal-itp/data-infra/tree/main/airflow/dags/create_external_tables) of the data-infra repository, you should check [BigQuery audit logs](https://cloud.google.com/bigquery/docs/reference/auditlogs/#data_access_data_access) to see whether people are querying the external tables that rely on the deprecation candidate bucket (and if so, eliminate it from the deprecation list).
   - Post in `#data-warehouse-devs` and any other relevant channels in Slack (this may vary by domain; for example, if investigating a bucket related to GTFS quality, you may post in `#gtfs-quality`). Ask whether anybody knows of ongoing use of the bucket(s) in question. If there are identifiable stakeholders who aren't active in Slack, like external research partners, reach out to them directly.

4. For each bucket that hasn't been removed from the deprecation list via the investigation in the last step, [label the bucket as "deprecated: true"](https://cloud.google.com/storage/docs/using-bucket-labels) and remove access for any relevant automated users or user groups (we want to intentionally break automated access so that errors occur if the bucket's data was still being referenced undetected). Since we use [uniform bucket-level access](https://cloud.google.com/storage/docs/uniform-bucket-level-access) for the vast majority of our buckets, removing access is [a simple operation on the Permissions page of the bucket](https://cloud.google.com/storage/docs/access-control/using-iam-permissions#bucket-remove). For buckets that have fine-grained access control, those permissions changes need to be made for each object in the bucket via those objects' [Access Control Lists](https://cloud.google.com/storage/docs/access-control/lists). In either case, once permissions changes have been made, inform stakeholders about the newly deprecated buckets via `#data-warehouse-devs` and other relevant channels, and monitor for two weeks for any new code or process breakages related to the changed bucket permissions.

5. After two weeks is up, take the most relevant option of the following two:

   - For buckets that must be retained because they represent raw data that can't otherwise be recovered in the future, or for regulatory reasons, or because of potential for future research/analysis access, place a small README inside the bucket explaining its deprecation. After that, [convert the bucket and its objects to the Archive storage class](https://cloud.google.com/storage/docs/changing-storage-classes). Note that future access to Archive class objects will incur higher costs than access to objects in the standard storage class - the Archive storage class is intended for data that will not be accessed frequently, and is cost-optimized for lower costs of storage and higher costs of access. Additionally, if Archive class objects are deleted after less than a year of Archive class storage, the Google Cloud project will still be billed for one year of storage, which is the minimum billable storage duration for Achive class objects.
   - For buckets that do not need to be kept long term, like out-of-use transformations of raw data that can be recreated if necessary from the corresponding raw data, simply delete the bucket.
